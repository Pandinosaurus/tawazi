{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#usage","title":"Usage","text":""},{"location":"#classes-decorators","title":"Classes &amp; decorators","text":"<p>In Tawazi, there 3 Classes that will be manipulated by the user:</p> <ol> <li><code>ExecNode</code>: a wrapper around a function. <code>ExecNode</code> can be executed inside a <code>DAG</code>. <code>ExecNode</code> can take arguments and return values to be used as arguments in other <code>ExecNode</code>s.</li> <li><code>DAG</code>: a wrapper around a function that defines a dag dependency. This function should only contain calls to <code>ExecNode</code>s (you can not call normal Python functions inside a <code>DAG</code>!)</li> <li><code>DAGExecution</code>: an instance related to <code>DAG</code> for advanced usage. It can execute a <code>DAG</code> and keeps information about the last execution. It allows checking all <code>ExecNode</code>s results, running subgraphs, caching <code>DAG</code> executions and more (c.f. section below for usage documentation).</li> </ol> <p>Decorators are provided to create the previous classes:</p> <ol> <li><code>@xn</code>: creates <code>ExecNode</code> from a function.</li> <li><code>@dag</code>: creates <code>DAG</code> from a function.</li> </ol>"},{"location":"#basic-usage","title":"basic usage","text":"<pre><code>from tawazi import xn, dag\n@xn\ndef incr(x):\nreturn x + 1\n# incr is no longer a function\n# incr became a `LazyExecNode` which is a subclass of `ExecNode`.\nprint(type(incr))\n@xn\ndef decr(x):\nreturn x - 1\n@xn\ndef display(x):\nprint(x)\n@dag\ndef pipeline(x):\nx_lo = decr(x)\nx_hi = incr(x)\ndisplay(x_hi)\ndisplay(x_lo)\n# pipeline is no longer a function\n# pipeline became a `DAG`\nprint(type(pipeline))\n# `DAG` can be executed, they behave the same as the original function without decorators.\npipeline(0)\n</code></pre> <p>By default, calling <code>ExecNode</code> outside of a <code>DAG</code> describing function will raise an error. </p> <p>However, the user can control this behavior by setting the environment variable <code>TAWAZI_EXECNODE_OUTSIDE_DAG_BEHAVIOR</code> to:</p> <ol> <li><code>\"error\"</code>: raise an error if an <code>ExecNode</code> is called outside of <code>DAG</code> description (default)</li> <li><code>\"warning\"</code>: raise a warning if an <code>ExecNode</code> is called outside of <code>DAG</code> description, but execute the wrapped function anyway</li> <li><code>\"ignore\"</code>: execute the wrapped function anyway.</li> </ol> <p>This way, <code>ExecNode</code> can still be called outside of a <code>DAG</code>. It will raise a warning.</p> <pre><code># set environment variable to warning\nfrom tawazi import cfg\ncfg.TAWAZI_EXECNODE_OUTSIDE_DAG_BEHAVIOR = \"warning\"\ndisplay('Hello World!')\n# &lt;stdin&gt;:1: UserWarning: Invoking LazyExecNode display ~ | &lt;0x7fdc03d4ebb0&gt; outside of a `DAG`. Executing wrapped function instead of describing dependency.\n# prints Hello World!\n</code></pre> <p>This makes it possible - in some cases - to debug your code outside Tawazi's scheduler and see the data movements between different <code>ExecNode</code>s. Simply remove <code>@dag</code> from the <code>pipeline</code> function and run it again.</p> <pre><code>@dag\ndef pipeline(x):\nx_lo = decr(x)\nx_hi = incr(x)\ndisplay(x_hi)\ndisplay(x_lo)\nreturn x\nassert pipeline(10) == 10\n#@dag  # comment out the decorator\ndef pipeline(x):\nx_lo = decr(x)\nx_hi = incr(x)  # put breakpoint here and debug!\ndisplay(x_hi)\ndisplay(x_lo)\nreturn x\nassert pipeline(10) == 10\n</code></pre>"},{"location":"#parallelism","title":"Parallelism","text":"<p>You can use Tawazi to make your non CPU-Bound code Faster</p> <p><pre><code>from time import sleep, time\n@xn\ndef a():\nprint(\"Function 'a' is running\", flush=True)\nsleep(1)\nreturn \"A\"\n@xn\ndef b():\nprint(\"Function 'b' is running\", flush=True)\nsleep(1)\nreturn \"B\"\n@xn\ndef c(a, b):\nprint(\"Function 'c' is running\", flush=True)\nprint(f\"Function 'c' received {a} from 'a' &amp; {b} from 'b'\", flush=True)\nreturn f\"{a} + {b} = C\"\n@dag(max_concurrency=2)\ndef pipeline():\nres_a = a()\nres_b = b()\nres_c = c(res_a, res_b)\nreturn res_c\nt0 = time()\n# executing the dag takes a single line of code\nres = pipeline()\nexecution_time = time() - t0\nassert execution_time &lt; 1.5  # a() and b() are executed in parallel\nprint(f\"Graph execution took {execution_time:.2f} seconds\")\nprint(f\"res = {res}\")\n</code></pre> As you can see, the execution time of pipeline takes less than 2 seconds, which means that some part of the code ran in parallel to the other</p>"},{"location":"#dag-is-like-a-normal-function","title":"<code>DAG</code> is like a normal function","text":"<p>You can pass in arguments to the pipeline and get returned results back like normal functions:</p> <p><pre><code>from tawazi import xn, dag\n@xn\ndef xn1(i):\nreturn i+1\n@xn\ndef xn2(i, j=1):\nreturn i + j + 1\n@dag\ndef pipeline(i=0):\nres1 = xn1(i)\nres2 = xn2(res1)\nreturn res2\n# run pipeline with default parameters\nassert pipeline() == 3\n# run pipeline with passed parameters\nassert pipeline(1) == 4\n</code></pre> Currently you can not pass in named parameters to the <code>DAG</code> (will be supported in future releases). (This should not be confused with passing keyworded arguments to <code>ExecNode</code>s which is possible)</p> <p>You can return multiple values from a pipeline via tuples, lists or dicts (depth of 1).</p> <pre><code>@dag\ndef pipeline_tuple():\nreturn xn1(1), xn2(1)\nassert pipeline_tuple() == (2, 3)\n@dag\ndef pipeline_list():\nreturn [xn1(1), xn2(2)]\nassert pipeline_list() == [2, 4]\n@dag\ndef pipeline_dict():\nreturn {\"foo\": xn1(1), \"bar\": xn2(3)}\nassert pipeline_dict() == {\"foo\": 2, \"bar\": 5}\n</code></pre> <p>You can return multiple values from <code>ExecNode</code>s:</p> <ol> <li>Either via Python <code>Tuple</code>s and <code>List</code>s but you will have to specify the unpacking number</li> </ol> <p><pre><code>@xn(unpack_to=4)\ndef replicate_tuple(val):\nreturn (val, val + 1, val + 2, val + 3)\n@xn(unpack_to=4)\ndef replicate_list(val):\nreturn [val, val + 1, val + 2, val + 3]\n@dag\ndef pipeline():\nv1, v2, v3, v4 = replicate_tuple(1)\nv5, v6, v7, v8 = replicate_list(v4)\nreturn v1, v2, v3, v4, v5, v6, v7, v8\nassert pipeline() == (1, 2, 3, 4, 4, 5, 6, 7)\n</code></pre> 2. Or via indexing (<code>Dict</code> or <code>List</code> etc.):</p> <p><pre><code>@xn\ndef gen_dict(val):\nreturn {\"k1\": val, \"k2\": \"2\", \"nested_list\": [1 ,11, 3]}\n@xn\ndef gen_list(val):\nreturn [val, val + 1, val + 2, val + 3]\n@xn\ndef incr(val):\nreturn val + 1\n@dag\ndef pipeline(val):\nd = gen_dict(val)\nl = gen_list(d[\"k1\"])\ninc_val = incr(l[0])\ninc_val_2 = incr(d[\"nested_list\"][1])\nreturn d, l, inc_val, inc_val_2\nd, l, inc_val, inc_val_2 = pipeline(123)\nassert d == {\"k1\": 123, \"k2\": \"2\", \"nested_list\": [1 ,11, 3]}\nassert l == [123, 124, 125, 126]\nassert inc_val == 124\nassert inc_val_2 == 12\n</code></pre> This makes the <code>DAG</code> usage as close to using the original pipeline function as possible.</p>"},{"location":"#setup-execnodes","title":"Setup <code>ExecNode</code>s","text":"<p>Setup <code>ExecNode</code>s have their results cached in the <code>DAG</code> instance. This means that they run once per <code>DAG</code> instance. These can be used to load large constant data from Disk (Machine Learning Models, Large CSV files, initialization of a resource, prewarming etc.)</p> <p><pre><code>LARGE_DATA = \"Long algorithm to generate Constant Data\"\n@xn(setup=True)\ndef setop():\nglobal setop_counter\nsetop_counter += 1\nreturn LARGE_DATA\n@xn\ndef my_print(arg):\nprint(arg)\nreturn arg\n@dag\ndef pipeline():\ncst_data = setop()\nlarge_data = my_print(cst_data)\nreturn large_data\nsetop_counter = 0\n# create another instance because setup ExecNode result is cached inside the instance\nassert LARGE_DATA == pipeline()\nassert setop_counter == 1\nassert LARGE_DATA == pipeline()\nassert setop_counter == 1 # setop_counter is skipped the second time pipe1 is invoked\n</code></pre> if you want to re-run the setup <code>ExecNode</code>, you have to redeclare the <code>DAG</code> or deepcopy the original <code>DAG</code> instance before executing it.</p> <p><pre><code>from copy import deepcopy\n@dag\ndef pipeline():\ncst_data = setop()\nlarge_data = my_print(cst_data)\nreturn large_data\nsetop_counter = 0\nassert LARGE_DATA == deepcopy(pipeline)()\nassert setop_counter == 1\nassert LARGE_DATA ==  deepcopy(pipeline)()\nassert setop_counter == 2\n</code></pre> You can run the setup <code>ExecNode</code>s alone:</p> <p><pre><code>@dag\ndef pipeline():\ncst_data = setop()\nlarge_data = my_print(cst_data)\nreturn large_data\npipeline.setup()\n</code></pre> The goal of having setup <code>ExecNode</code> is to load only the necessary resources when a subgraph is executed. Here is an example demonstrating it:</p> <pre><code>from pprint import PrettyPrinter\n@xn(setup=True)\ndef setop1():\nreturn \"large data 1\"\n@xn(setup=True)\ndef setop2():\nreturn \"large data 2\"\n@xn\ndef print_xn(val):\nprint(val)\n@xn\ndef pprint_xn(val):\nPrettyPrinter(indent=4).pprint(val)\n@dag\ndef pipeline():\ndata1 = setop1()\ndata2 = setop2()\nprint_xn(data1)\npprint_xn(data2)\nreturn data1, data2\nexec_ = pipeline.executor(target_nodes=[\"print_xn\"])\n# Notice how the execution of the subgraph doesn't run the setop1 `ExecNode`.\n# This makes development of your complex pipeline faster by loading only the necessary resources.\nassert (\"large data 1\", None) == exec_()\n</code></pre>"},{"location":"#debug-execnode","title":"Debug <code>ExecNode</code>","text":"<p>You can Make Debug <code>ExecNode</code>s that will only run if <code>RUN_DEBUG_NODES</code> env variable is set. These can be visualization <code>ExecNode</code>s for example or some complicated Assertions that helps you debug problems when needed that are hostile to the production environment (they consume too much computation time):</p> <pre><code>@xn\ndef a(i):\nreturn i + 1\n@xn(debug=True)\ndef print_debug(i):\nglobal debug_has_run\ndebug_has_run = True\nprint(\"DEBUG: \", i)\n@dag\ndef pipe():\ni = a(0)\nprint_debug(i)\nreturn i\ndebug_has_run = False\npipe()\nassert debug_has_run == False\n</code></pre>"},{"location":"#advanced-usage","title":"Advanced Usage","text":""},{"location":"#tag","title":"Tag","text":"<p>A tag is a user defined identifier for an <code>ExecNode</code>. Every <code>ExecNode</code> can have a no tag or multiple _tag_s.</p> <p>You can tag an <code>ExecNode</code> with an <code>str</code>. You can also tag it using multiple tags (<code>Tuple[str]</code>)</p> <p><pre><code>@xn(tag=\"twinkle toes\")\ndef a():\nprint(\"I am tough\")\n@xn\ndef b():\nprint(\"I am normal\")\n@dag\ndef pipeline():\na()\nb()\npipeline()\nxn_a, = pipeline.get_nodes_by_tag(\"twinkle toes\")\n</code></pre> You can do whatever you want with this ExecNode:</p> <ol> <li>like looking in its arguments</li> <li>setting its priority</li> <li>changing it to become a debug <code>ExecNode</code></li> </ol> <p>WARNING: This is an advanced usage. Your methods might break more often with Tawazi releases because <code>ExecNode</code> is an internal Object. Please use with care</p> <p>You can have multiple <code>Tag</code>s for the same <code>ExecNode</code> and the same <code>Tag</code> for multiple <code>ExecNode</code>s:</p> <pre><code>@xn(tag=(\"twinkle\", \"toes\"))\ndef a():\nprint(\"I am tough\")\n@xn(tag=\"twinkle\")\ndef b():\nprint(\"I am light\")\n@dag\ndef pipeline():\na()\nb()\nxn_a, xn_b = pipeline.get_nodes_by_tag(\"twinkle\")\n</code></pre> <p>You can even tag a specific call of an ExecNode:</p> <pre><code>@xn\ndef stub_xn(i):\nreturn i\n@xn(tag=\"c_node\")\ndef print_xn(i):\nprint(i)\n@dag\ndef pipeline():\nb()\nhello = stub_xn(\"hello\")\nprint_xn(hello)\ngoodbye = stub_xn(\"goodbye\")\nprint_xn(goodbye, twz_tag=\"byebye\")\npipeline()\n# multiple nodes can have the same tag!\nxns_bye = pipeline.get_nodes_by_tag(\"byebye\")\n</code></pre> <p>This will be useful if you want to run a subgraph (cf. the next paragraph). It will also be useful if you want to access result of a specific ExecNode after an Execution</p>"},{"location":"#dagexecution","title":"<code>DAGExecution</code>","text":"<p><code>DAGExecution</code> is a class that contains a reference to the <code>DAG</code>. It is used to run a <code>DAG</code> or a subgraph of the <code>DAG</code>. After execution, the results of <code>ExecNode</code>s are stored in the <code>DAGExecution</code> instance. Hence you can access intermediate results after execution.</p> <p><pre><code>from tawazi import DAGExecution\n# construct a DAGExecution from a DAG by doing\ndx = DAGExecution(pipeline)\n# or\ndx = pipeline.executor()\n</code></pre> You can run a subgraph of your pipeline: Make a <code>DAGExecution</code> from your <code>DAG</code> and use <code>target_nodes</code> parameter to specify which <code>ExecNode</code> to run.</p> <p>The DAG will execute until the specified <code>ExecNode</code>s and all other <code>ExecNode</code>s will be skipped.</p> <p><pre><code>pipe_exec = pipeline.executor(target_nodes=[b])\npipe_exec()\n</code></pre> You can use the <code>__qualname__</code> of the decorated function as an Identifier.</p> <p><pre><code>pipe_exec = pipeline.executor(target_nodes=[\"b\"])\npipe_exec()\n</code></pre> You can use the tag of an ExecNode</p> <p><pre><code>pipe_exec = pipeline.executor(target_nodes=[\"c_node\"])\npipe_exec()\n</code></pre> You can use the calling tag to distinguish the 1st call of g from the 2nd call!</p> <p><pre><code>pipe_exec = pipeline.executor(target_nodes=[\"byebye\"])\npipe_exec()\n</code></pre> You can even pass in the <code>ExecNode</code>s themselves and mix identifiers types</p> <pre><code>pipe_exec = pipeline.executor(target_nodes=[\"b\", xns_bye[0]])\npipe_exec()\n</code></pre> <p>Warning</p> <p>Because <code>DAGExecution</code> instances are mutable, they are non thread-safe. This is unlike <code>DAG</code> which is ThreadSafe</p>"},{"location":"#basic-operations-between-nodes","title":"Basic Operations between nodes","text":"<p><code>UsageExecNode</code>s implements almost all basic operations (addition, substraction, ...).</p> <p><pre><code>@xn\ndef gen_data(x):\nreturn (x + 1) ** 2\n@xn(debug=True)\ndef my_print(x):\nprint(f\"x={x}\")\n@dag\ndef pipe(x, y):\nx,y = gen_data(x), gen_data(y)\nreturn -x, -y, x+y, x*y\nassert pipe(1, 2) == (-4, -9, 13, 36)\n</code></pre> It's not possible to support logical operations <code>and</code>, <code>or</code> and <code>not</code> since <code>__bool__</code> should always return a <code>boolean</code>. during the dependency description phase, all <code>xn</code> decorated functions return <code>UsageExecNode</code>. However bitwise logical operators are implemented so that bitwise <code>&amp;</code> can be used inside a <code>DAG</code>.</p> <p><code>&amp;</code>, <code>|</code> vs <code>and</code>, <code>or</code></p> <p><code>&amp;</code> and <code>|</code> have different behavior than <code>and</code> and <code>or</code> in python. <code>and</code> and <code>or</code> are short-circuiting while <code>&amp;</code> and <code>|</code> are not. This is because <code>and</code> and <code>or</code> are logical operators while <code>&amp;</code> and <code>|</code> are bitwise operators.</p> <pre><code>@dag\ndef pipe(x: bool, y: bool):\nreturn x &amp; y, x | y\nassert pipe(True, False) == (False, True)\n</code></pre>"},{"location":"#conditional-execution","title":"Conditional Execution","text":"<p>Currently, conditional statements are not supported in <code>DAG</code>. However, <code>ExecNode</code>s can be executed conditionally by passing a <code>bool</code> to the added parameter <code>twz_active</code>. This parameter can be a constant or a result of an execution of other <code>ExecNode</code> in the <code>DAG</code>. Since basic boolean operations are implemented on <code>UsageExecNode</code>s, you can use bitwise operations (<code>&amp;</code>, <code>or</code>) to simulate <code>and</code>, <code>or</code>; however this is not recommended, please use the provided <code>and_</code>, <code>or_</code> and <code>not_</code> <code>ExecNode</code>s instead.</p> <pre><code>from tawazi import and_\n@xn\ndef f1(x):\nreturn x ** 2 + 1\n@xn\ndef f2(x):\nreturn x ** 2 - 1\n@xn\ndef f3(x):\nreturn x ** 3\n@xn\ndef wrap_dict(x):\nreturn {\"value\": x}\n@dag\ndef pipe(x):\nv1 = f1(x, twz_active=x &gt; 0)  # equivalent to if x &gt; 0: v1 = f1(x)\nv2 = f2(x, twz_active=x &lt; 0)  # equivalent to if x &lt; 0: v2 = f2(x)\n# you can also write `v3 = f3(x, twz_active=(x &gt; 1) &amp; (x &gt; 0))`\nv3 = f3(x, twz_active=and_(x &gt; 1, x &gt; 0))\n# subsequent usages only execute if the previous `ExecNode` is executed otherwise returns None\nr1 = wrap_dict(v1)\nr2 = wrap_dict(v2)\nr3 = wrap_dict(v3)\nreturn r1, r2, r3\nassert pipe(-1) == (None, {\"value\": 0}, None)\nassert pipe(2) == ({\"value\": 5}, None, {\"value\": 8})\nassert pipe(0) == (None, None, None)\n</code></pre>"},{"location":"#fine-control-of-parallel-execution","title":"Fine Control of Parallel Execution","text":"<ol> <li>You can control which node is preferred to run 1st when mutliple <code>ExecNode</code>s are available for execution. This can be achieved through modifications of <code>priority</code> of the <code>ExecNode</code>.</li> <li>You can even make an <code>ExecNode</code> run alone (i.e. without allowing other ExecNodes to execute in parallel to it). This can be helpful if you write code that is not thread-safe or use a library that is not thread-safe in a certain <code>ExecNode</code>. This is achieved by setting the <code>is_sequential</code> parameter to <code>True</code> for the <code>ExecNode</code> in question. The default value is set via the environment variable <code>TAWAZI_IS_SEQUENTIAL</code> (c.f. <code>tawazi.config</code>).</li> <li>You can control the behavior of the <code>DAG</code> in case an <code>ExecNode</code> fails:   a. <code>\"strict\"</code>: stop execution of the DAG   b. <code>\"all-children\"</code>:  stop the execution of the all successors   c. <code>\"permissive\"</code>: continue the execution of the whole DAG</li> </ol> <pre><code>from time import sleep, time\nfrom tawazi import xn, dag\n@xn\ndef a():\nprint(\"Function 'a' is running\", flush=True)\nsleep(1)\nreturn \"A\"\n# optionally configure each ExecNode using the decorator:\n# is_sequential = True to prevent ExecNode from running in parallel with other ExecNodes\n# priority to choose the ExecNode in the next execution phase\n@xn(is_sequential=True, priority=10)\ndef b():\nprint(\"Function 'b' is running\", flush=True)\nsleep(1)\nreturn \"B\"\n@xn\ndef c(a, arg_b):\nprint(\"Function 'c' is running\", flush=True)\nprint(f\"Function 'c' received {a} from 'a' &amp; {arg_b} from 'b'\", flush=True)\nreturn f\"{a} + {arg_b} = C\"\n# optionally customize the DAG\n@dag(max_concurrency=2, behavior=\"strict\")\ndef deps_describer():\nres_a = a()\nres_b = b()\nres_c = c(res_a, res_b)\nreturn res_a, res_b, res_c\nt0 = time()\n# the dag instance is reusable.\n# This is recommended if you want to do the same computation multiple times\nres_a, res_b, res_c = deps_describer()\nexecution_time = time() - t0\nprint(f\"Graph execution took {execution_time:.2f} seconds\")\nassert res_a == \"A\"\nassert res_b == \"B\"\nassert res_c == \"A + B = C\"\n</code></pre>"},{"location":"#dag-composition","title":"DAG Composition","text":"<p>Experimental</p> <p>You can compose a sub-<code>DAG</code> from your original <code>DAG</code>. This is useful if you want to reuse a part of your <code>DAG</code>. Using the <code>DAG.compose</code> method, you provide the inputs and the outputs of the composed sub-<code>DAG</code>. Order is kept.</p> <p>Inputs and outputs are communicated via either the <code>ExecNode</code> reference or the tag/id of the <code>ExecNode</code>. Any ambiguity will raise an <code>Error</code>.</p> <p>Warning</p> <p>All necessary inputs should be provided to produce the desired outputs. Otherwise an <code>ValueError</code> is raised.</p> <pre><code>@xn\ndef add(x, y):\nreturn x + y\n@xn\ndef mul(x, y):\nreturn x * y\n@dag\ndef pipe(x, y, z, w):\nv1 = add(1, x, twz_tag=\"add_v1\")\nv2 = add(v1, y)\nv3 = add(v2, z, twz_tag=\"add_v3\")\nv4 = mul(v3, w)\nreturn v4\nassert pipe(2,3,4,5) == 50\n# declare a sub-dag that only depends on v1, y, z and produces v3\nsub_dag = pipe.compose(inputs=[\"add_v1\", \"pipe&gt;&gt;&gt;y\", \"pipe&gt;&gt;&gt;z\"], outputs=\"add_v3\")\nassert sub_dag(2,3,4) == 9\n# notice that for inputs, we provide the return value of the ExecNode (return value of ExecNode tagged \"add_v1\")\n# but for the outputs, we indicate the the ExecNode whose return value must return.\n</code></pre>"},{"location":"#resource-usage-for-execution","title":"Resource Usage for Execution","text":"<p>You can control the resource used to run a specific <code>ExecNode</code>. By default, all <code>ExecNode</code>s run in threads inside a ThreadPoolExecutor. This can be changed by setting the <code>resource</code> parameter of the <code>ExecNode</code>. Currently only two values are supported: </p> <ol> <li>\"thread\": Run the <code>ExecNode</code> inside a thread (default).</li> <li>\"main-thread\": Run the <code>ExecNode</code> inside the main thread without Pickling the data to pass it to the threads etc.</li> </ol> <p><pre><code>from tawazi import Resource\nimport threading\n@xn(resource=Resource.main_thread)\ndef run_in_main_thread(main_thread_id):\nassert main_thread_id == threading.get_ident()\nprint(f\"I am running in the main thread with thread id {threading.get_ident()}\")\n@xn(resource=Resource.thread)\ndef run_in_thread(main_thread_id):\nassert main_thread_id != threading.get_ident()\nprint(f\"I am running in a thread with thread id {threading.get_ident()}\")\n@dag\ndef dag_with_resource(main_thread_id):\nrun_in_main_thread(main_thread_id)\nrun_in_thread(main_thread_id)\ndag_with_resource(threading.get_ident())\n</code></pre> You can also set the default resource for all <code>ExecNode</code>s by setting the environment variable <code>TAWAZI_DEFAULT_RESOURCE</code> to either \"thread\" or \"main-thread\".</p>"},{"location":"#limitations","title":"Limitations","text":"<ol> <li>All code inside a dag descriptor function must be either an @xn decorated functions calls and arguments passed arguments. Otherwise the behavior of the DAG might be unpredictable</li> <li>Because the main function serves only for the purpose of describing the dependencies, the code that it executes should only describe dependencies. Hence when debugging your code, it will be impossible to view the data movement inside this function. However, you can debug code inside of a node.</li> <li>MyPy typing is supported. However, for certain cases it is not currently possible to support typing: (<code>twz_tag</code>, <code>twz_active</code>, <code>twz_unpack_to</code> etc.). This is because of pep612's limitation for concatenating-keyword-parameters. As a workaround, you can currently add <code>**kwargs</code> to your original function declaring that it can accept keyworded arguments. However none of the inline tawazi specific parameters (<code>twz_*</code>) parameters will be passed to your function:</li> </ol> <pre><code>@xn\ndef f(x: int):\nreturn x\nf(2)  # works\ntry:\n# f() got an unexpected keyword argument 'twz_tag'\nf(2, twz_tag=\"twinkle\")  # fails (mypy error)\nexcept TypeError:\n...\n@xn\ndef f_with_kwargs(x: int, **kwargs):\nreturn x\nf_with_kwargs(2, twz_tag=\"toes\")  # works\n</code></pre>"},{"location":"DAGExecution/","title":"DAGExecution","text":"<p>         Bases: <code>Generic[P, RVDAG]</code></p> <p>A disposable callable instance of a DAG.</p> <p>It holds information about the last execution. Hence it is not threadsafe. It might be reusable, however it is not recommended to reuse an instance of DAGExecutor!.</p> Source code in <code>tawazi/_dag/dag.py</code> <pre><code>class DAGExecution(Generic[P, RVDAG]):\n\"\"\"A disposable callable instance of a DAG.\n    It holds information about the last execution. Hence it is not threadsafe.\n    It might be reusable, however it is not recommended to reuse an instance of DAGExecutor!.\n    \"\"\"\ndef __init__(\nself,\ndag: DAG[P, RVDAG],\n*,\ntarget_nodes: Optional[Sequence[Alias]] = None,\nexclude_nodes: Optional[Sequence[Alias]] = None,\ncache_deps_of: Optional[Sequence[Alias]] = None,\ncache_in: str = \"\",\nfrom_cache: str = \"\",\ncall_id: Optional[str] = None,\n):\n\"\"\"Constructor.\n        Args:\n            dag (DAG): The attached DAG.\n            target_nodes (Optional[List[Alias]]): The leave ExecNodes to execute.\n                If None will execute all ExecNodes.\n                Defaults to None.\n            exclude_nodes (Optional[List[Alias]]): The leave ExecNodes to exclude.\n                If None will exclude all ExecNodes.\n                Defaults to None.\n            cache_deps_of (Optional[List[Alias]]): cache all the dependencies of these nodes.\n                This option can not be used together with target_nodes nor exclude_nodes.\n            cache_in (str):\n                the path to the file where the execution should be cached.\n                The path should end in `.pkl`.\n                Will skip caching if `cache_in` is Falsy.\n                Will raise PickleError if any of the values passed around in the DAG is not pickleable.\n                Defaults to \"\".\n            from_cache (str):\n                the path to the file where the execution should be loaded from.\n                The path should end in `.pkl`.\n                Will skip loading from cache if `from_cache` is Falsy.\n                Defaults to \"\".\n            call_id (Optional[str]): identification of the current execution.\n                This will be inserted into thread_name_prefix while executing the threadPool.\n                It will be used in the future for identifying the execution inside Processes etc.\n        \"\"\"\n# todo: Maybe we can support .dill to extend the possibilities of the exchanged values, but this won't solve the whole problem\nself.dag = dag\nself.target_nodes = target_nodes\nself.exclude_nodes = exclude_nodes\nself.cache_deps_of = cache_deps_of\nself.cache_in = cache_in\nself.from_cache = from_cache\n# NOTE: from_cache is orthogonal to cache_in which means that if cache_in is set at the same time as from_cache.\n#  in this case the DAG will be loaded from_cache and the results will be saved again to the cache_in file.\nself.call_id = call_id\n# get the leaves ids to execute in case of a subgraph\nself.target_nodes = target_nodes\nself.exclude_nodes = exclude_nodes\nself.xn_dict: Dict[Identifier, ExecNode] = {}\nself.results: Dict[Identifier, Any] = {}\nself._construct_dynamic_attributes()\nself.executed = False\ndef _construct_dynamic_attributes(self) -&gt; None:\nself.graph = self._make_graph()\nself.scheduled_nodes = self.graph.nodes\ndef _make_graph(self) -&gt; nx.DiGraph:\n\"\"\"Make the graph of the execution.\n        This method is called only once per instance.\n        \"\"\"\n# logic parts\nif self.cache_deps_of is not None:\nreturn self.dag._make_subgraph(self.cache_deps_of)\nreturn self.dag._make_subgraph(self.target_nodes, self.exclude_nodes)\n@property\ndef cache_in(self) -&gt; str:\n\"\"\"The path to the file where the execution should be cached.\n        Returns:\n            str: The path to the file where the execution should be cached.\n        \"\"\"\nreturn self._cache_in\n@cache_in.setter\ndef cache_in(self, cache_in: str) -&gt; None:\nif cache_in and not cache_in.endswith(\".pkl\"):\nraise ValueError(\"cache_in should end with.pkl\")\nself._cache_in = cache_in\n@property\ndef from_cache(self) -&gt; str:\n\"\"\"Get the file path from which the cached execution should be loaded.\n        Returns:\n            str: the file path of the cached execution\n        \"\"\"\nreturn self._from_cache\n@from_cache.setter\ndef from_cache(self, from_cache: str) -&gt; None:\nif from_cache and not from_cache.endswith(\".pkl\"):\nraise ValueError(\"from_cache should end with.pkl\")\nself._from_cache = from_cache\n@property\ndef cache_deps_of(self) -&gt; Optional[Sequence[Alias]]:\n\"\"\"Cache all the dependencies of these nodes.\n        Returns:\n            Optional[List[Alias]]: List of Aliases passed to cache_deps_of while instantiating DAGExecution\n        \"\"\"\nreturn self._cache_deps_of\n@cache_deps_of.setter\ndef cache_deps_of(self, cache_deps_of: Optional[Sequence[Alias]]) -&gt; None:\nif (\nself.target_nodes is not None or self.exclude_nodes is not None\n) and cache_deps_of is not None:\nraise ValueError(\n\"cache_deps_of can not be used together with target_nodes or exclude_nodes\"\n)\nself._cache_deps_of = cache_deps_of\n# we need to reimplement the public methods of DAG here in order to have a constant public interface\n# getters\ndef get_nodes_by_tag(self, tag: Tag) -&gt; List[ExecNode]:\n\"\"\"Get all the nodes with the given tag.\n        Args:\n            tag (Tag): tag of ExecNodes in question\n        Returns:\n            List[ExecNode]: corresponding ExecNodes\n        \"\"\"\nif self.executed:\nreturn [ex_n for ex_n in self.xn_dict.values() if ex_n.tag == tag]\nreturn self.dag.get_nodes_by_tag(tag)\ndef get_node_by_id(self, id_: Identifier) -&gt; ExecNode:\n\"\"\"Get node with the given id.\n        Args:\n            id_ (Identifier): id of the ExecNode\n        Returns:\n            ExecNode: Corresponding ExecNode\n        \"\"\"\n# TODO: ? catch the keyError and\n#   help the user know the id of the ExecNode by pointing to documentation!?\nif self.executed:\nreturn self.xn_dict[id_]\nreturn self.dag.get_node_by_id(id_)\ndef setup(self) -&gt; None:\n\"\"\"Does the same thing as DAG.setup. However the `target_nodes` and `exclude_nodes` are taken from the DAGExecution's initization.\"\"\"\n# TODO: handle the case where cache_deps_of is provided instead of target_nodes and exclude_nodes\n#  in which case the deps_of might have a setup node themselves which should not run.\n#  This is an edge case though that is not important to handle at the current moment.\nself.dag.setup(target_nodes=self.target_nodes, exclude_nodes=self.exclude_nodes)\ndef __call__(self, *args: P.args, **kwargs: P.kwargs) -&gt; RVDAG:\n\"\"\"Call the DAG.\n        Args:\n            *args: positional arguments to pass in to the DAG\n            **kwargs: keyword arguments to pass in to the DAG\n        Raises:\n            TawaziUsageError: if the DAGExecution has already been executed.\n        Returns:\n            RVDAG: the return value of the DAG's Execution\n        \"\"\"\nif self.executed:\nwarnings.warn(\"DAGExecution object's reuse is not recommended.\", stacklevel=2)\nself._construct_dynamic_attributes()\n# NOTE: *args will be ignored if self.from_cache is set!\ndag = self.dag\n# maybe call_id will be changed to Union[int, str].\n# Keep call_id as Optional[str] for now\ncall_id = self.call_id if self.call_id is not None else \"\"\n# 1. copy the ExecNodes\ncall_xn_dict = dag._make_call_xn_dict(*args)\nif self.from_cache:\nwith open(self.from_cache, \"rb\") as f:\ncached_results = pickle.load(f)  # noqa: S301\n# set the result for the ExecNode that were previously executed\n# this will make them skip execution inside the scheduler\nfor id_, result in cached_results.items():\ncall_xn_dict[id_].result = result\n# 2. Execute the scheduler\nself.xn_dict = dag._execute(self.graph, call_xn_dict, call_id)\nself.results = {xn.id: xn.result for xn in self.xn_dict.values()}\n# 3. cache in the graph results\nif self.cache_in:\nPath(self.cache_in).parent.mkdir(parents=True, exist_ok=True)\nwith open(self.cache_in, \"wb\") as f:\n# NOTE: we are currently only storing the results of the execution,\n#  this means that the configuration of the ExecNodes are lost!\n#  But this is ok since it should not change between executions!\n#  for example, a setup ExecNode should stay a setup ExecNode between caching in the results and reading back the cached results\n#  the same goes for the DAG itself, the behavior when an error is encountered &amp; its concurrency will be controlled via the constructor\nif self.cache_deps_of is not None:\nnon_cacheable_ids: Set[Identifier] = set()\nfor aliases in self.cache_deps_of:\nids = self.dag._alias_to_ids(aliases)\nnon_cacheable_ids = non_cacheable_ids.union(ids)\nto_cache_results = {\nid_: res\nfor id_, res in self.results.items()\nif id_ not in non_cacheable_ids\n}\nelse:\nto_cache_results = self.results\npickle.dump(\nto_cache_results, f, protocol=pickle.HIGHEST_PROTOCOL, fix_imports=False\n)\n# TODO: make DAGExecution reusable but do not guarantee ThreadSafety!\nself.executed = True\n# 3. extract the returned value/values\nreturn dag._get_return_values(self.xn_dict)  # type: ignore[return-value]\n</code></pre>"},{"location":"DAGExecution/#tawazi._dag.dag.DAGExecution.__init__","title":"<code>__init__(dag, *, target_nodes=None, exclude_nodes=None, cache_deps_of=None, cache_in='', from_cache='', call_id=None)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>dag</code> <code>DAG</code> <p>The attached DAG.</p> required <code>target_nodes</code> <code>Optional[List[Alias]]</code> <p>The leave ExecNodes to execute. If None will execute all ExecNodes. Defaults to None.</p> <code>None</code> <code>exclude_nodes</code> <code>Optional[List[Alias]]</code> <p>The leave ExecNodes to exclude. If None will exclude all ExecNodes. Defaults to None.</p> <code>None</code> <code>cache_deps_of</code> <code>Optional[List[Alias]]</code> <p>cache all the dependencies of these nodes. This option can not be used together with target_nodes nor exclude_nodes.</p> <code>None</code> <code>cache_in</code> <code>str</code> <p>the path to the file where the execution should be cached. The path should end in <code>.pkl</code>. Will skip caching if <code>cache_in</code> is Falsy. Will raise PickleError if any of the values passed around in the DAG is not pickleable. Defaults to \"\".</p> <code>''</code> <code>from_cache</code> <code>str</code> <p>the path to the file where the execution should be loaded from. The path should end in <code>.pkl</code>. Will skip loading from cache if <code>from_cache</code> is Falsy. Defaults to \"\".</p> <code>''</code> <code>call_id</code> <code>Optional[str]</code> <p>identification of the current execution. This will be inserted into thread_name_prefix while executing the threadPool. It will be used in the future for identifying the execution inside Processes etc.</p> <code>None</code> Source code in <code>tawazi/_dag/dag.py</code> <pre><code>def __init__(\nself,\ndag: DAG[P, RVDAG],\n*,\ntarget_nodes: Optional[Sequence[Alias]] = None,\nexclude_nodes: Optional[Sequence[Alias]] = None,\ncache_deps_of: Optional[Sequence[Alias]] = None,\ncache_in: str = \"\",\nfrom_cache: str = \"\",\ncall_id: Optional[str] = None,\n):\n\"\"\"Constructor.\n    Args:\n        dag (DAG): The attached DAG.\n        target_nodes (Optional[List[Alias]]): The leave ExecNodes to execute.\n            If None will execute all ExecNodes.\n            Defaults to None.\n        exclude_nodes (Optional[List[Alias]]): The leave ExecNodes to exclude.\n            If None will exclude all ExecNodes.\n            Defaults to None.\n        cache_deps_of (Optional[List[Alias]]): cache all the dependencies of these nodes.\n            This option can not be used together with target_nodes nor exclude_nodes.\n        cache_in (str):\n            the path to the file where the execution should be cached.\n            The path should end in `.pkl`.\n            Will skip caching if `cache_in` is Falsy.\n            Will raise PickleError if any of the values passed around in the DAG is not pickleable.\n            Defaults to \"\".\n        from_cache (str):\n            the path to the file where the execution should be loaded from.\n            The path should end in `.pkl`.\n            Will skip loading from cache if `from_cache` is Falsy.\n            Defaults to \"\".\n        call_id (Optional[str]): identification of the current execution.\n            This will be inserted into thread_name_prefix while executing the threadPool.\n            It will be used in the future for identifying the execution inside Processes etc.\n    \"\"\"\n# todo: Maybe we can support .dill to extend the possibilities of the exchanged values, but this won't solve the whole problem\nself.dag = dag\nself.target_nodes = target_nodes\nself.exclude_nodes = exclude_nodes\nself.cache_deps_of = cache_deps_of\nself.cache_in = cache_in\nself.from_cache = from_cache\n# NOTE: from_cache is orthogonal to cache_in which means that if cache_in is set at the same time as from_cache.\n#  in this case the DAG will be loaded from_cache and the results will be saved again to the cache_in file.\nself.call_id = call_id\n# get the leaves ids to execute in case of a subgraph\nself.target_nodes = target_nodes\nself.exclude_nodes = exclude_nodes\nself.xn_dict: Dict[Identifier, ExecNode] = {}\nself.results: Dict[Identifier, Any] = {}\nself._construct_dynamic_attributes()\nself.executed = False\n</code></pre>"},{"location":"DAGExecution/#tawazi._dag.dag.DAGExecution.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Call the DAG.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>P.args</code> <p>positional arguments to pass in to the DAG</p> <code>()</code> <code>**kwargs</code> <code>P.kwargs</code> <p>keyword arguments to pass in to the DAG</p> <code>{}</code> <p>Raises:</p> Type Description <code>TawaziUsageError</code> <p>if the DAGExecution has already been executed.</p> <p>Returns:</p> Name Type Description <code>RVDAG</code> <code>RVDAG</code> <p>the return value of the DAG's Execution</p> Source code in <code>tawazi/_dag/dag.py</code> <pre><code>def __call__(self, *args: P.args, **kwargs: P.kwargs) -&gt; RVDAG:\n\"\"\"Call the DAG.\n    Args:\n        *args: positional arguments to pass in to the DAG\n        **kwargs: keyword arguments to pass in to the DAG\n    Raises:\n        TawaziUsageError: if the DAGExecution has already been executed.\n    Returns:\n        RVDAG: the return value of the DAG's Execution\n    \"\"\"\nif self.executed:\nwarnings.warn(\"DAGExecution object's reuse is not recommended.\", stacklevel=2)\nself._construct_dynamic_attributes()\n# NOTE: *args will be ignored if self.from_cache is set!\ndag = self.dag\n# maybe call_id will be changed to Union[int, str].\n# Keep call_id as Optional[str] for now\ncall_id = self.call_id if self.call_id is not None else \"\"\n# 1. copy the ExecNodes\ncall_xn_dict = dag._make_call_xn_dict(*args)\nif self.from_cache:\nwith open(self.from_cache, \"rb\") as f:\ncached_results = pickle.load(f)  # noqa: S301\n# set the result for the ExecNode that were previously executed\n# this will make them skip execution inside the scheduler\nfor id_, result in cached_results.items():\ncall_xn_dict[id_].result = result\n# 2. Execute the scheduler\nself.xn_dict = dag._execute(self.graph, call_xn_dict, call_id)\nself.results = {xn.id: xn.result for xn in self.xn_dict.values()}\n# 3. cache in the graph results\nif self.cache_in:\nPath(self.cache_in).parent.mkdir(parents=True, exist_ok=True)\nwith open(self.cache_in, \"wb\") as f:\n# NOTE: we are currently only storing the results of the execution,\n#  this means that the configuration of the ExecNodes are lost!\n#  But this is ok since it should not change between executions!\n#  for example, a setup ExecNode should stay a setup ExecNode between caching in the results and reading back the cached results\n#  the same goes for the DAG itself, the behavior when an error is encountered &amp; its concurrency will be controlled via the constructor\nif self.cache_deps_of is not None:\nnon_cacheable_ids: Set[Identifier] = set()\nfor aliases in self.cache_deps_of:\nids = self.dag._alias_to_ids(aliases)\nnon_cacheable_ids = non_cacheable_ids.union(ids)\nto_cache_results = {\nid_: res\nfor id_, res in self.results.items()\nif id_ not in non_cacheable_ids\n}\nelse:\nto_cache_results = self.results\npickle.dump(\nto_cache_results, f, protocol=pickle.HIGHEST_PROTOCOL, fix_imports=False\n)\n# TODO: make DAGExecution reusable but do not guarantee ThreadSafety!\nself.executed = True\n# 3. extract the returned value/values\nreturn dag._get_return_values(self.xn_dict)  # type: ignore[return-value]\n</code></pre>"},{"location":"DAGExecution/#tawazi._dag.dag.DAGExecution.setup","title":"<code>setup()</code>","text":"<p>Does the same thing as DAG.setup. However the <code>target_nodes</code> and <code>exclude_nodes</code> are taken from the DAGExecution's initization.</p> Source code in <code>tawazi/_dag/dag.py</code> <pre><code>def setup(self) -&gt; None:\n\"\"\"Does the same thing as DAG.setup. However the `target_nodes` and `exclude_nodes` are taken from the DAGExecution's initization.\"\"\"\n# TODO: handle the case where cache_deps_of is provided instead of target_nodes and exclude_nodes\n#  in which case the deps_of might have a setup node themselves which should not run.\n#  This is an edge case though that is not important to handle at the current moment.\nself.dag.setup(target_nodes=self.target_nodes, exclude_nodes=self.exclude_nodes)\n</code></pre>"},{"location":"DAGExecution/#tawazi._dag.dag.DAGExecution.get_node_by_id","title":"<code>get_node_by_id(id_)</code>","text":"<p>Get node with the given id.</p> <p>Parameters:</p> Name Type Description Default <code>id_</code> <code>Identifier</code> <p>id of the ExecNode</p> required <p>Returns:</p> Name Type Description <code>ExecNode</code> <code>ExecNode</code> <p>Corresponding ExecNode</p> Source code in <code>tawazi/_dag/dag.py</code> <pre><code>def get_node_by_id(self, id_: Identifier) -&gt; ExecNode:\n\"\"\"Get node with the given id.\n    Args:\n        id_ (Identifier): id of the ExecNode\n    Returns:\n        ExecNode: Corresponding ExecNode\n    \"\"\"\n# TODO: ? catch the keyError and\n#   help the user know the id of the ExecNode by pointing to documentation!?\nif self.executed:\nreturn self.xn_dict[id_]\nreturn self.dag.get_node_by_id(id_)\n</code></pre>"},{"location":"DAGExecution/#tawazi._dag.dag.DAGExecution.get_nodes_by_tag","title":"<code>get_nodes_by_tag(tag)</code>","text":"<p>Get all the nodes with the given tag.</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>Tag</code> <p>tag of ExecNodes in question</p> required <p>Returns:</p> Type Description <code>List[ExecNode]</code> <p>List[ExecNode]: corresponding ExecNodes</p> Source code in <code>tawazi/_dag/dag.py</code> <pre><code>def get_nodes_by_tag(self, tag: Tag) -&gt; List[ExecNode]:\n\"\"\"Get all the nodes with the given tag.\n    Args:\n        tag (Tag): tag of ExecNodes in question\n    Returns:\n        List[ExecNode]: corresponding ExecNodes\n    \"\"\"\nif self.executed:\nreturn [ex_n for ex_n in self.xn_dict.values() if ex_n.tag == tag]\nreturn self.dag.get_nodes_by_tag(tag)\n</code></pre>"},{"location":"dag/","title":"DAG","text":"<p>         Bases: <code>Generic[P, RVDAG]</code></p> <p>Data Structure containing ExecNodes with interdependencies.</p> <p>Please do not instantiate this class directly. Use the decorator <code>@dag</code> instead.</p> The ExecNodes can be executed in parallel with the following restrictions <ul> <li>Limited number of threads.</li> <li>Parallelization constraint of each ExecNode (is_sequential attribute)</li> </ul> Source code in <code>tawazi/_dag/dag.py</code> <pre><code>class DAG(Generic[P, RVDAG]):\n\"\"\"Data Structure containing ExecNodes with interdependencies.\n    Please do not instantiate this class directly. Use the decorator `@dag` instead.\n    The ExecNodes can be executed in parallel with the following restrictions:\n        * Limited number of threads.\n        * Parallelization constraint of each ExecNode (is_sequential attribute)\n    \"\"\"\ndef __init__(\nself,\nexec_nodes: Dict[Identifier, ExecNode],\ninput_uxns: List[UsageExecNode],\nreturn_uxns: ReturnUXNsType,\nmax_concurrency: int = 1,\nbehavior: ErrorStrategy = ErrorStrategy.strict,\n):\n\"\"\"Constructor of the DAG. Should not be called directly. Instead use the `dag` decorator.\n        Args:\n            exec_nodes: all the ExecNodes\n            input_uxns: all the input UsageExecNodes\n            return_uxns: the return UsageExecNodes. These can be of various types: None, a single value, tuple, list, dict.\n            max_concurrency: the maximal number of threads running in parallel\n            behavior: specify the behavior if an ExecNode raises an Error. Three option are currently supported:\n                1. DAG.STRICT: stop the execution of all the DAG\n                2. DAG.ALL_CHILDREN: do not execute all children ExecNodes, and continue execution of the DAG\n                2. DAG.PERMISSIVE: continue execution of the DAG and ignore the error\n        \"\"\"\nself.max_concurrency = max_concurrency\nself.behavior = behavior\nself.return_uxns = return_uxns\nself.input_uxns = input_uxns\n# ExecNodes can be shared between Graphs, their call signatures might also be different\n# NOTE: maybe this should be transformed into a property because there is a deepcopy for node_dict...\n#  this means that there are different ExecNodes that are hanging arround in the same instance of the DAG\nself.node_dict = exec_nodes\n# Compute all the tags in the DAG to reduce overhead during computation\nself.tagged_nodes = defaultdict(list)\nfor xn in self.node_dict.values():\nif xn.tag:\nif isinstance(xn.tag, Tag):\nself.tagged_nodes[xn.tag].append(xn)\n# isinstance(xn.tag, tuple):\nelse:\nfor t in xn.tag:\nself.tagged_nodes[t].append(xn)\n# Might be useful in the future\nself.node_dict_by_name: Dict[str, ExecNode] = {\nexec_node.__name__: exec_node for exec_node in self.node_dict.values()\n}\nself.graph_ids = self._build_graph()\nself.bckrd_deps = {\nid_: list(self.graph_ids.predecessors(xn.id)) for id_, xn in self.node_dict.items()\n}\nself.frwrd_deps = {\nid_: list(self.graph_ids.successors(xn.id)) for id_, xn in self.node_dict.items()\n}\n# calculate the sum of priorities of all recursive children\nself._assign_compound_priority()\n# make a valid execution sequence to run sequentially if needed\ntopological_order = self.graph_ids.topologically_sorted\nself.exec_node_sequence = [self.node_dict[xn_id] for xn_id in topological_order]\nself._validate()\n@property\ndef max_concurrency(self) -&gt; int:\n\"\"\"Maximal number of threads running in parallel. (will change!).\"\"\"\nreturn self._max_concurrency\n@max_concurrency.setter\ndef max_concurrency(self, value: int) -&gt; None:\n\"\"\"Set the maximal number of threads running in parallel.\n        Args:\n            value (int): maximum number of threads running in parallel\n        Raises:\n            ValueError: if value is not a positive integer\n        \"\"\"\nif not isinstance(value, int):\nraise ValueError(\"max_concurrency must be an int\")\nif value &lt; 1:\nraise ValueError(\"Invalid maximum number of threads! Must be a positive integer\")\nself._max_concurrency = value\n# getters\ndef get_nodes_by_tag(self, tag: Tag) -&gt; List[ExecNode]:\n\"\"\"Get the ExecNodes with the given tag.\n        Note: the returned ExecNode is not modified by any execution!\n            This means that you can not get the result of its execution via `DAG.get_nodes_by_tag(&lt;tag&gt;).result`.\n            In order to do that, you need to make a DAGExecution and then call `DAGExecution.get_nodes_by_tag(&lt;tag&gt;).result`, which will contain the results.\n        Args:\n            tag (Any): tag of the ExecNodes\n        Returns:\n            List[ExecNode]: corresponding ExecNodes\n        \"\"\"\nif isinstance(tag, Tag):\nreturn self.tagged_nodes[tag]\nraise TypeError(f\"tag {tag} must be of Tag type. Got {type(tag)}\")\ndef get_node_by_id(self, id_: Identifier) -&gt; ExecNode:\n\"\"\"Get the ExecNode with the given id.\n        Note: the returned ExecNode is not modified by any execution!\n            This means that you can not get the result of its execution via `DAG.get_node_by_id(&lt;id&gt;).result`.\n            In order to do that, you need to make a DAGExecution and then call `DAGExecution.get_node_by_id(&lt;id&gt;).result`, which will contain the results.\n        Args:\n            id_ (Identifier): id of the ExecNode\n        Returns:\n            ExecNode: corresponding ExecNode\n        \"\"\"\n# TODO: ? catch the keyError and\n#   help the user know the id of the ExecNode by pointing to documentation!?\nreturn self.node_dict[id_]\ndef _get_single_xn_by_alias(self, alias: Alias) -&gt; ExecNode:\n\"\"\"Get the ExecNode corresponding to the given Alias.\n        Args:\n            alias (Alias): the Alias to be resolved\n        Raises:\n            ValueError: if the Alias is not unique\n        Returns:\n            ExecNode: the ExecNode corresponding to the given Alias\n        \"\"\"\nxns = self._alias_to_ids(alias)\nif len(xns) &gt; 1:\nraise ValueError(\nf\"Alias {alias} is not unique. It points to {len(xns)} ExecNodes: {xns}\"\n)\nreturn self.node_dict[xns[0]]\n# TODO: get node by usage (the order of call of an ExecNode)\n# TODO: implement None for outputs to indicate a None output ? (this is not a prioritized feature)\n# TODO: implement ellipsis for composing for the input &amp; outputs\n# TODO: should we support kwargs when DAG.__call__ support kwargs?\n# TODO: Maybe insert an ID into DAG that is related to the dependency describing function !? just like ExecNode\n#  This will be necessary when we want to make a DAG containing DAGs besides ExecNodes\n# NOTE: by doing this, we create a new ExecNode for each input.\n#  Hence we loose all information related to the original ExecNode (tags, etc.)\n#  Maybe a better way to do this is to transform the original ExecNode into an ArgExecNode\ndef compose(self, inputs: Union[Alias, Sequence[Alias]], outputs: Union[Alias, Sequence[Alias]], **kwargs: Dict[str, Any]) -&gt; \"DAG\":  # type: ignore[type-arg]\n\"\"\"Compose a new DAG using inputs and outputs ExecNodes (Experimental).\n        All provided `Alias`es must point to unique `ExecNode`s. Otherwise ValueError is raised\n        The user is responsible to correctly specify inputs and outputs signature of the `DAG`.\n        * The inputs can be specified as a single `Alias` or a `Sequence` of `Alias`es.\n        * The outputs can be specified as a single `Alias` (a single value is returned)\n        or a `Sequence` of `Alias`es in which case a Tuple of the values are returned.\n        If outputs are specified as [], () is returned.\n        The syntax is the following:\n        &gt;&gt;&gt; from tawazi import dag, xn, DAG\n        &gt;&gt;&gt; from typing import Tuple, Any\n        &gt;&gt;&gt; @xn\n        ... def unwanted_xn() -&gt; int: return 42\n        &gt;&gt;&gt; @xn\n        ... def x(v: Any) -&gt; int: return int(v)\n        &gt;&gt;&gt; @xn\n        ... def y(v: Any) -&gt; str: return str(v)\n        &gt;&gt;&gt; @xn\n        ... def z(x: int, y: str) -&gt; float: return float(x) + float(y)\n        &gt;&gt;&gt; @dag\n        ... def pipe() -&gt; Tuple[int, float, int]:\n        ...     a = unwanted_xn()\n        ...     res = z(x(1), y(1))\n        ...     b = unwanted_xn()\n        ...     return a, res, b\n        &gt;&gt;&gt; composed_dag = pipe.compose([x, y], z)\n        &gt;&gt;&gt; assert composed_dag(1, 1) == 2.0\n        &gt;&gt;&gt; # composed_dag: DAG[[int, str], float] = pipe.compose([x, y], [z])  # optional typing of the returned DAG!\n        &gt;&gt;&gt; # assert composed_dag(1, 1) == 2.0  # type checked!\n        Args:\n            inputs (Alias | List[Alias]): the Inputs nodes whose results are provided.\n            outputs (Alias | List[Alias]): the Output nodes that must execute last, The ones that will generate results\n            **kwargs (Dict[str, Any]): additional arguments to be passed to the DAG's constructor\n        \"\"\"\n# what happens for edge cases ??\n# 1. if inputs are less than sufficient to produce outputs (-&gt; error)\n# 2. if inputs are more than sufficient to produce outputs (-&gt; warning)\n# 3. if inputs are successors of outputs (-&gt; error)\n# 5. if inputs are successors of inputs but predecessors of outputs\n# 1. inputs and outputs are overlapping (-&gt; error ambiguous ? maybe not)\n# 1. if inputs &amp; outputs are [] (-&gt; ())\n# 4. cst cubgraph inputs is [], outputs is not [] but contains constants (-&gt; works as expected)\n# 5. inputs is not [], outputs is [] same as 2.\n# 6. a subcase of the above (some inputs suffice to produce some of the outputs, but some inputs don't)\n# 7. advanced usage: if inputs contain ... (i.e. Ellipsis) in this case we must expand it to reach all the remaining XN in a smart manner\n#  we should keep the order of the inputs and outputs (future)\n# 8. what if some arguments have default values? should they be provided by the user?\n# 9. how to specify that arguments of the original DAG should be provided by the user? the user should provide the input's ID which is not a stable Alias yet\ndef _alias_or_aliases_to_ids(\nalias_or_aliases: Union[Alias, Sequence[Alias]]\n) -&gt; List[Identifier]:\nif isinstance(alias_or_aliases, str) or isinstance(alias_or_aliases, ExecNode):\nreturn [self._get_single_xn_by_alias(alias_or_aliases).id]\nreturn [self._get_single_xn_by_alias(a_id).id for a_id in alias_or_aliases]\ndef _raise_input_successor_of_input(pred: Identifier, succ: Set[Identifier]) -&gt; NoReturn:\nraise ValueError(\nf\"Input ExecNodes {succ} depend on Input ExecNode {pred}.\"\nf\"this is ambiguous. Remove either one of them.\"\n)\ndef _raise_missing_input(input_: Identifier) -&gt; NoReturn:\nraise ValueError(\nf\"ExecNode {input_} are not declared as inputs. \"\nf\"Either declare them as inputs or modify the requests outputs.\"\n)\ndef _alias_or_aliases_to_uxns(\nalias_or_aliases: Union[Alias, Sequence[Alias]]\n) -&gt; ReturnUXNsType:\nif isinstance(alias_or_aliases, str) or isinstance(alias_or_aliases, ExecNode):\nreturn UsageExecNode(self._get_single_xn_by_alias(alias_or_aliases).id)\nreturn tuple(\nUsageExecNode(self._get_single_xn_by_alias(a_id).id) for a_id in alias_or_aliases\n)\n# 1. get input ids and output ids.\n#  Alias should correspond to a single ExecNode,\n#  otherwise an ambiguous situation exists, raise error\nin_ids = _alias_or_aliases_to_ids(inputs)\nout_ids = _alias_or_aliases_to_ids(outputs)\n# 2.1 contains all the ids of the nodes that will be in the new DAG\nset_xn_ids = set(in_ids + out_ids)\n# 2.2 all ancestors of the inputs\nin_ids_ancestors: Set[Identifier] = self.graph_ids.ancestors_of_iter(in_ids)\n# 3. check edge cases\n# inputs should not be successors of inputs, otherwise (error)\n# and inputs should produce at least one of the outputs, otherwise (warning)\nfor i in in_ids:\n# if pred is ancestor of an input, raise error\nif i in in_ids_ancestors:\n_raise_input_successor_of_input(i, set_xn_ids)\n# if i doesn't produce any of the wanted outputs, raise a warning!\ndescendants: Set[Identifier] = nx.descendants(self.graph_ids, i)\nif descendants.isdisjoint(out_ids):\nwarnings.warn(\nf\"Input ExecNode {i} is not used to produce any of the requested outputs.\"\nf\"Consider removing it from the inputs.\",\nstacklevel=2,\n)\n# 4. collect necessary ExecNodes' IDS\n# 4.1 original DAG's inputs that don't contain default values.\n# used to detect missing inputs\ndag_inputs_ids = [\nuxn.id for uxn in self.input_uxns if self.node_dict[uxn.id].result is NoVal\n]\n# 4.2 define helper function\ndef _add_missing_deps(candidate_id: Identifier, set_xn_ids: Set[Identifier]) -&gt; None:\n\"\"\"Adds missing dependency to the set of ExecNodes that will be in the new DAG.\n            Note: uses nonlocal variable dag_inputs_ids\n            Args:\n                candidate_id (Identifier): candidate id of an `ExecNode` that will be in the new DAG\n                set_xn_ids (Set[Identifier]): Set of `ExecNode`s that will be in the new DAG\n            \"\"\"\npreds = self.graph_ids.predecessors(candidate_id)\nfor pred in preds:\nif pred not in set_xn_ids:\n# this candidate is necessary to produce the output,\n# it is an input to the original DAG\n# it is not provided as an input to the composed DAG\n# hence the user forgot to supply it! (raise error)\nif pred in dag_inputs_ids:\n_raise_missing_input(pred)\n# necessary intermediate dependency.\n# collect it in the set\nset_xn_ids.add(pred)\n_add_missing_deps(pred, set_xn_ids)\n# 4.3 add all required dependencies for each output\nfor o_id in out_ids:\n_add_missing_deps(o_id, set_xn_ids)\n# 5.1 copy the ExecNodes that will be in the composed DAG because\n#  maybe the composed DAG will modify them (e.g. change their tags)\n#  and we don't want to modify the original DAG\nxn_dict = {xn_id: copy(self.node_dict[xn_id]) for xn_id in set_xn_ids}\n# 5.2 change the inputs of the ExecNodes into ArgExecNodes\nfor xn_id, xn in xn_dict.items():\nif xn_id in in_ids:\nlogger.debug(\"changing Composed-DAG's input {} into ArgExecNode\", xn_id)\nxn.__class__ = ArgExecNode\nxn.exec_function = _make_raise_arg_error(\"composed\", xn.id)\n# eliminate all dependencies\nxn.args = []\nxn.kwargs = {}\n# 5.3 make the inputs and outputs UXNs for the composed DAG\nin_uxns = [UsageExecNode(xn_id) for xn_id in in_ids]\n# if a single value is returned make the output a single value\nout_uxns = _alias_or_aliases_to_uxns(outputs)\n# 6. return the composed DAG\n# ignore[arg-type] because the type of the kwargs is not known\nreturn DAG(xn_dict, in_uxns, out_uxns, **kwargs)  # type: ignore[arg-type]\ndef _build_graph(self) -&gt; DiGraphEx:\n\"\"\"Builds the graph and the sequence order for the computation.\n        Raises:\n            NetworkXUnfeasible: if the graph has cycles\n        \"\"\"\ngraph_ids = DiGraphEx()\n# 1. Make the graph\n# 1.1 add nodes\nfor id_ in self.node_dict.keys():\ngraph_ids.add_node(id_)\n# 1.2 add edges\nfor xn in self.node_dict.values():\nedges = [(dep.id, xn.id) for dep in xn.dependencies]\ngraph_ids.add_edges_from(edges)\n# 2. Validate the DAG: check for circular dependencies\ncycle = graph_ids._find_cycle()\nif cycle:\nraise NetworkXUnfeasible(\nf\"the product contains at least a circular dependency: {cycle}\"\n)\nreturn graph_ids\ndef _validate(self) -&gt; None:\ninput_ids = [uxn.id for uxn in self.input_uxns]\n# validate setup ExecNodes\nfor xn in self.node_dict.values():\nif xn.setup and any(dep.id in input_ids for dep in xn.dependencies):\nraise TawaziUsageError(\nf\"The ExecNode {xn} takes as parameters one of the DAG's input parameter\"\n)\n# future validations...\ndef _assign_compound_priority(self) -&gt; None:\n\"\"\"Assigns a compound priority to all nodes in the graph.\n        The compound priority is the sum of the priorities of all children recursively.\n        \"\"\"\n# 1. deepcopy graph_ids because it will be modified (pruned)\ngraph_ids = deepcopy(self.graph_ids)\nleaf_ids = graph_ids.leaf_nodes\n# 2. assign the compound priority for all the remaining nodes in the graph:\n# Priority assignment happens by epochs:\n# 2.1. during every epoch, we assign the compound priority for the parents of the current leaf nodes\n# 2.2. at the end of every epoch, we trim the graph from its leaf nodes;\n#       hence the previous parents become the new leaf nodes\nwhile len(graph_ids) &gt; 0:\n# Epoch level\nfor leaf_id in leaf_ids:\nleaf_node = self.node_dict[leaf_id]\nfor parent_id in self.bckrd_deps[leaf_id]:\n# increment the compound_priority of the parent node by the leaf priority\nparent_node = self.node_dict[parent_id]\nparent_node.compound_priority += leaf_node.compound_priority\n# trim the graph from its leaf nodes\ngraph_ids.remove_node(leaf_id)\n# assign the new leaf nodes\nleaf_ids = graph_ids.leaf_nodes\ndef draw(self, k: float = 0.8, display: bool = True, t: int = 3) -&gt; None:\n\"\"\"Draws the Networkx directed graph.\n        Args:\n            k (float): parameter for the layout of the graph, the higher, the further the nodes apart. Defaults to 0.8.\n            display (bool): display the layout created. Defaults to True.\n            t (int): time to display in seconds. Defaults to 3.\n        \"\"\"\nimport matplotlib.pyplot as plt\n# TODO: use graphviz instead! it is much more elegant\npos = nx.spring_layout(self.graph_ids, seed=42069, k=k, iterations=20)\nnx.draw(self.graph_ids, pos, with_labels=True)\nif display:\nplt.ion()\nplt.show()\ntime.sleep(t)\nplt.close()\ndef _execute(\nself,\ngraph: DiGraphEx,\nmodified_node_dict: Optional[Dict[str, ExecNode]] = None,\ncall_id: str = \"\",\n) -&gt; Dict[Identifier, Any]:\nreturn execute(\nnode_dict=self.node_dict,\nmax_concurrency=self.max_concurrency,\nbehavior=self.behavior,\ngraph=graph,\nmodified_node_dict=modified_node_dict,\ncall_id=call_id,\n)\ndef _alias_to_ids(self, alias: Alias) -&gt; List[Identifier]:\n\"\"\"Extract an ExecNode ID from an Alias (Tag, ExecNode ID or ExecNode).\n        Args:\n            alias (Alias): an Alias (Tag, ExecNode ID or ExecNode)\n        Returns:\n            The corresponding ExecNode IDs\n        Raises:\n            ValueError: if a requested ExecNode is not found in the DAG\n            TawaziTypeError: if the Type of the identifier is not Tag, Identifier or ExecNode\n        \"\"\"\nif isinstance(alias, ExecNode):\nif alias.id not in self.node_dict:\nraise ValueError(f\"ExecNode {alias} not found in DAG\")\nreturn [alias.id]\n# todo: do further validation for the case of the tag!!\nif isinstance(alias, (Identifier, tuple)):\n# if leaves_identification is not ExecNode, it can be either\n#  1. a Tag (Highest priority in case an id with the same value exists)\nnodes = self.tagged_nodes.get(alias)\nif nodes:\nreturn [node.id for node in nodes]\n#  2. or a node id!\nif isinstance(alias, Identifier) and alias in self.node_dict:\nnode = self.get_node_by_id(alias)\nreturn [node.id]\nraise ValueError(\nf\"node or tag {alias} not found in DAG.\\n\"\nf\" Available nodes are {self.node_dict}.\\n\"\nf\" Available tags are {list(self.tagged_nodes.keys())}\"\n)\nraise TawaziTypeError(\n\"target_nodes must be of type ExecNode, \"\nf\"str or tuple identifying the node but provided {alias}\"\n)\n# NOTE: this function is named wrongly!\ndef _get_target_ids(self, target_nodes: Sequence[Alias]) -&gt; List[Identifier]:\n\"\"\"Get the ids of ExecNodes corresponding to target_nodes.\n        Args:\n            target_nodes (Optional[List[Alias]]): list of a ExecNode Aliases that the user might provide to run a subgraph\n        Returns:\n            List[Identifier]: Leaf ExecNodes' Identities\n        \"\"\"\nreturn list(chain(*(self._alias_to_ids(alias) for alias in target_nodes)))\ndef _extend_leaves_ids_debug_xns(self, leaves_ids: List[Identifier]) -&gt; List[Identifier]:\nnew_debug_xn_discovered = True\nwhile new_debug_xn_discovered:\nnew_debug_xn_discovered = False\nfor id_ in leaves_ids:\nfor successor_id in self.frwrd_deps[id_]:\nis_successor_debug = self.node_dict[successor_id].debug\nif successor_id not in leaves_ids and is_successor_debug:\n# a new debug XN has been discovered!\nnew_debug_xn_discovered = True\npreds_of_succs_ids = [xn_id for xn_id in self.bckrd_deps[successor_id]]\nif set(preds_of_succs_ids).issubset(set(leaves_ids)):\n# this new XN can run by only running the current leaves_ids\nleaves_ids.append(successor_id)\nreturn leaves_ids\ndef setup(\nself,\ntarget_nodes: Optional[Sequence[Alias]] = None,\nexclude_nodes: Optional[Sequence[Alias]] = None,\n) -&gt; None:\n\"\"\"Run the setup ExecNodes for the DAG.\n        If target_nodes are provided, run only the necessary setup ExecNodes, otherwise will run all setup ExecNodes.\n        NOTE: `DAG` arguments should not be passed to setup ExecNodes.\n            Only pass in constants or setup `ExecNode`s results.\n        Args:\n            target_nodes (Optional[List[XNId]], optional): The ExecNodes that the user aims to use in the DAG.\n                This might include setup or non setup ExecNodes. If None is provided, will run all setup ExecNodes. Defaults to None.\n            exclude_nodes (Optional[List[XNId]], optional): The ExecNodes that the user aims to exclude from the DAG.\n                The user is responsible for ensuring that the overlapping between the target_nodes and exclude_nodes is logical.\n        \"\"\"\n# 1. select all setup ExecNodes\n#  do not copy the setup nodes because we want them to be modified per DAG instance!\nall_setup_nodes = {\nid_: xn\nfor id_, xn in self.node_dict.items()\nif xn.setup or (isinstance(xn, ArgExecNode) and xn.executed)\n}\n# 2. if target_nodes is not provided run all setup ExecNodes\nif target_nodes is None:\ntarget_ids = list(all_setup_nodes.keys())\ngraph = self._make_subgraph(target_ids, exclude_nodes)\nelse:\n# 2.1 the leaves_ids that the user wants to execute\n#  however they might contain non setup nodes... so we should extract all the nodes ids\n#  that must be run in order to run the target_nodes ExecNodes\n#  afterwards we can remove the non setup nodes\ntarget_ids = self._get_target_ids(target_nodes)\n# 2.2 filter non setup ExecNodes\ngraph = self._make_subgraph(target_ids, exclude_nodes)\nids_to_remove = [id_ for id_ in graph if id_ not in all_setup_nodes]\nfor id_ in ids_to_remove:\ngraph.remove_node(id_)\n# TODO: handle debug XNs!\nself._execute(graph, all_setup_nodes)\ndef executor(self, **kwargs: Any) -&gt; \"DAGExecution[P, RVDAG]\":\n\"\"\"Generates a DAGExecution for the DAG.\n        Args:\n            **kwargs (Any): keyword arguments to be passed to DAGExecution's constructor\n        Returns:\n            DAGExecution: an executor for the DAG\n        \"\"\"\nreturn DAGExecution(self, **kwargs)\ndef _make_subgraph(\nself,\ntarget_nodes: Optional[Sequence[Alias]] = None,\nexclude_nodes: Optional[Sequence[Alias]] = None,\n) -&gt; nx.DiGraph:\ngraph = deepcopy(self.graph_ids)\nif target_nodes is not None:\ntarget_ids = self._get_target_ids(target_nodes)\ngraph.subgraph_leaves(target_ids)\nif exclude_nodes is not None:\nexclude_ids = list(chain(*(self._alias_to_ids(alias) for alias in exclude_nodes)))\nfor id_ in exclude_ids:\n# maybe previously removed by :\n# 1. not being inside the subgraph\n# 2. being a successor of an excluded node\nif id_ in graph:\ngraph.remove_recursively(id_)\nif target_nodes and exclude_nodes:\nfor id_ in target_ids:\nif id_ not in graph:\nraise TawaziUsageError(\nf\"target_nodes include {id_} which is removed by exclude_nodes: {exclude_ids}, \"\nf\"please verify that they don't overlap in a non logical way!\"\n)\n# handle debug nodes\nif cfg.RUN_DEBUG_NODES:\nleaves_ids = graph.leaf_nodes\n# after extending leaves_ids, we should do a recheck because this might recreate another debug-able XN...\ntarget_ids = self._extend_leaves_ids_debug_xns(leaves_ids)\n# extend the graph with the debug XNs\n# This is not efficient but it is ok since we are debugging the code anyways\ndebug_graph = deepcopy(self.graph_ids)\ndebug_graph.subgraph_leaves(list(graph.nodes) + target_ids)\ngraph = debug_graph\n# 3. clean all debug XNs if they shouldn't run!\nelse:\nto_remove = [id_ for id_ in graph if self.node_dict[id_].debug]\nfor id_ in to_remove:\ngraph.remove_node(id_)\nreturn graph\ndef __call__(self, *args: P.args, **kwargs: P.kwargs) -&gt; RVDAG:\n\"\"\"Execute the DAG scheduler via a similar interface to the function that describes the dependencies.\n        Note: Currently kwargs are not supported.\n            They will supported soon!\n        Args:\n            *args (P.args): arguments to be passed to the call of the DAG\n            **kwargs (P.kwargs): keyword arguments to be passed to the call of the DAG\n        Returns:\n            RVDAG: return value of the DAG's execution\n        Raises:\n            TawaziUsageError: kwargs are passed\n        \"\"\"\nif kwargs:\nraise TawaziUsageError(f\"currently DAG does not support keyword arguments: {kwargs}\")\n# 1. generate the subgraph to be executed\ngraph = self._make_subgraph()\n# 2. copy the ExecNodes\ncall_xn_dict = self._make_call_xn_dict(*args)\n# 3. Execute the scheduler\nall_nodes_dict = self._execute(graph, call_xn_dict)\n# 4. extract the returned value/values\nreturn self._get_return_values(all_nodes_dict)  # type: ignore[return-value]\ndef _make_call_xn_dict(self, *args: Any) -&gt; Dict[Identifier, ExecNode]:\n\"\"\"Generate the calling ExecNode dict.\n        This is a dict containing ExecNodes that will be executed (hence modified) by the DAG scheduler.\n        This takes into consideration:\n         1. deep copying the ExecNodes\n         2. filling the arguments of the call\n         3. skipping the copy for setup ExecNodes\n        Args:\n            *args (Any): arguments to be passed to the call of the DAG\n        Returns:\n            Dict[Identifier, ExecNode]: The modified ExecNode dict which will be executed by the DAG scheduler.\n        Raises:\n            TypeError: If called with an invalid number of arguments\n        \"\"\"\n# 1. deepcopy the node_dict because it will be modified by the DAG's execution\ncall_xn_dict = copy_non_setup_xns(self.node_dict)\n# 2. parse the input arguments of the pipeline\n# 2.1 default valued arguments can be skipped and not provided!\n# note: if not enough arguments are provided then the code will fail\n# inside the DAG's execution through the raise_err lambda\nif args:\n# 2.2 can't provide more than enough arguments\nif len(args) &gt; len(self.input_uxns):\nraise TypeError(\nf\"The DAG takes a maximum of {len(self.input_uxns)} arguments. {len(args)} arguments provided\"\n)\n# 2.3 modify ExecNodes corresponding to input ArgExecNodes\nfor ind_arg, arg in enumerate(args):\nnode_id = self.input_uxns[ind_arg].id\ncall_xn_dict[node_id].result = arg\nreturn call_xn_dict\ndef _get_return_values(self, xn_dict: Dict[Identifier, ExecNode]) -&gt; RVTypes:\n\"\"\"Extract the return value/values from the output of the DAG's scheduler!\n        Args:\n            xn_dict (Dict[Identifier, ExecNode]): Modified ExecNodes returned by the DAG's scheduler\n        Raises:\n            TawaziTypeError: if the type of the return value is not compatible with RVTypes\n        Returns:\n            RVTypes: the actual values extracted from xn_dict\n        \"\"\"\nreturn_uxns = self.return_uxns\nif return_uxns is None:\nreturn None\nif isinstance(return_uxns, UsageExecNode):\nreturn return_uxns.result(xn_dict)\nif isinstance(return_uxns, (tuple, list)):\ngen = (ren_uxn.result(xn_dict) for ren_uxn in return_uxns)\nif isinstance(return_uxns, tuple):\nreturn tuple(gen)\nif isinstance(return_uxns, list):\nreturn list(gen)\nif isinstance(return_uxns, dict):\nreturn {key: ren_uxn.result(xn_dict) for key, ren_uxn in return_uxns.items()}\nraise TawaziTypeError(\"Return type for the DAG can only be a single value, Tuple or List\")\n# NOTE: this function should be used in case there was a bizarre behavior noticed during\n#   the execution of the DAG via DAG.execute(...)\ndef _safe_execute(\nself,\n*args: Any,\ntarget_nodes: Optional[List[Alias]] = None,\nexclude_nodes: Optional[List[Alias]] = None,\n) -&gt; Any:\n\"\"\"Execute the ExecNodes in topological order without priority in for loop manner for debugging purposes (Experimental).\n        Args:\n            *args (Any): Positional arguments passed to the DAG\n            target_nodes (Optional[List[Alias]]): the ExecNodes that should be considered to construct the subgraph\n            exclude_nodes (Optional[List[Alias]]): the ExecNodes that shouldn't run\n        Returns:\n            Any: the result of the execution of the DAG.\n             If an ExecNode returns a value in the DAG but is not executed, it will return None.\n        \"\"\"\n# 1. make the graph_ids to be executed!\ngraph = self._make_subgraph(target_nodes, exclude_nodes)\n# 2. make call_xn_dict that will be modified\ncall_xn_dict = self._make_call_xn_dict(*args)\n# 3. deep copy the node_dict to store the results in each node\nfor xn_id in graph.topologically_sorted:\n# only execute ExecNodes that are part of the subgraph\ncall_xn_dict[xn_id]._execute(call_xn_dict)\n# 4. make returned values\nreturn self._get_return_values(call_xn_dict)\ndef config_from_dict(self, config: Dict[str, Any]) -&gt; None:\n\"\"\"Allows reconfiguring the parameters of the nodes from a dictionary.\n        Args:\n            config (Dict[str, Any]): the dictionary containing the config\n                example: {\"nodes\": {\"a\": {\"priority\": 3, \"is_sequential\": True}}, \"max_concurrency\": 3}\n        Raises:\n            ValueError: if two nodes are configured by the provided config (which is ambiguous)\n        \"\"\"\ndef _override_node_config(n: ExecNode, cfg: Dict[str, Any]) -&gt; bool:\nif \"is_sequential\" in cfg:\nn.is_sequential = cfg[\"is_sequential\"]\nif \"priority\" in cfg:\nn.priority = cfg[\"priority\"]\nreturn True\nreturn False\nprio_flag = False\nvisited: Dict[str, Any] = {}\nif \"nodes\" in config:\nfor alias, conf_node in config[\"nodes\"].items():\nids_ = self._alias_to_ids(alias)\nfor node_id in ids_:\nif node_id not in visited:\nnode = self.get_node_by_id(node_id)\nnode_prio_flag = _override_node_config(node, conf_node)\nprio_flag = node_prio_flag or prio_flag  # keep track of flag\nelse:\nraise ValueError(\nf\"trying to set two configs for node {node_id}.\\n 1) {visited[node_id]}\\n 2) {conf_node}\"\n)\nvisited[node_id] = conf_node\nif \"max_concurrency\" in config:\nself.max_concurrency = config[\"max_concurrency\"]\nif prio_flag:\n# if we changed the priority of some nodes we need to recompute the compound prio\nself._assign_compound_priority()\ndef config_from_yaml(self, config_path: str) -&gt; None:\n\"\"\"Allows reconfiguring the parameters of the nodes from a YAML file.\n        Args:\n            config_path: the path to the YAML file\n        \"\"\"\nwith open(config_path) as f:\nyaml_config = yaml.load(f, Loader=_UniqueKeyLoader)  # noqa: S506\nself.config_from_dict(yaml_config)\ndef config_from_json(self, config_path: str) -&gt; None:\n\"\"\"Allows reconfiguring the parameters of the nodes from a JSON file.\n        Args:\n            config_path: the path to the JSON file\n        \"\"\"\nwith open(config_path) as f:\njson_config = json.load(f)\nself.config_from_dict(json_config)\n</code></pre>"},{"location":"dag/#tawazi._dag.dag.DAG.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Execute the DAG scheduler via a similar interface to the function that describes the dependencies.</p> Currently kwargs are not supported. <p>They will supported soon!</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>P.args</code> <p>arguments to be passed to the call of the DAG</p> <code>()</code> <code>**kwargs</code> <code>P.kwargs</code> <p>keyword arguments to be passed to the call of the DAG</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>RVDAG</code> <code>RVDAG</code> <p>return value of the DAG's execution</p> <p>Raises:</p> Type Description <code>TawaziUsageError</code> <p>kwargs are passed</p> Source code in <code>tawazi/_dag/dag.py</code> <pre><code>def __call__(self, *args: P.args, **kwargs: P.kwargs) -&gt; RVDAG:\n\"\"\"Execute the DAG scheduler via a similar interface to the function that describes the dependencies.\n    Note: Currently kwargs are not supported.\n        They will supported soon!\n    Args:\n        *args (P.args): arguments to be passed to the call of the DAG\n        **kwargs (P.kwargs): keyword arguments to be passed to the call of the DAG\n    Returns:\n        RVDAG: return value of the DAG's execution\n    Raises:\n        TawaziUsageError: kwargs are passed\n    \"\"\"\nif kwargs:\nraise TawaziUsageError(f\"currently DAG does not support keyword arguments: {kwargs}\")\n# 1. generate the subgraph to be executed\ngraph = self._make_subgraph()\n# 2. copy the ExecNodes\ncall_xn_dict = self._make_call_xn_dict(*args)\n# 3. Execute the scheduler\nall_nodes_dict = self._execute(graph, call_xn_dict)\n# 4. extract the returned value/values\nreturn self._get_return_values(all_nodes_dict)  # type: ignore[return-value]\n</code></pre>"},{"location":"dag/#tawazi._dag.dag.DAG.config_from_dict","title":"<code>config_from_dict(config)</code>","text":"<p>Allows reconfiguring the parameters of the nodes from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any]</code> <p>the dictionary containing the config example: {\"nodes\": {\"a\": {\"priority\": 3, \"is_sequential\": True}}, \"max_concurrency\": 3}</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if two nodes are configured by the provided config (which is ambiguous)</p> Source code in <code>tawazi/_dag/dag.py</code> <pre><code>def config_from_dict(self, config: Dict[str, Any]) -&gt; None:\n\"\"\"Allows reconfiguring the parameters of the nodes from a dictionary.\n    Args:\n        config (Dict[str, Any]): the dictionary containing the config\n            example: {\"nodes\": {\"a\": {\"priority\": 3, \"is_sequential\": True}}, \"max_concurrency\": 3}\n    Raises:\n        ValueError: if two nodes are configured by the provided config (which is ambiguous)\n    \"\"\"\ndef _override_node_config(n: ExecNode, cfg: Dict[str, Any]) -&gt; bool:\nif \"is_sequential\" in cfg:\nn.is_sequential = cfg[\"is_sequential\"]\nif \"priority\" in cfg:\nn.priority = cfg[\"priority\"]\nreturn True\nreturn False\nprio_flag = False\nvisited: Dict[str, Any] = {}\nif \"nodes\" in config:\nfor alias, conf_node in config[\"nodes\"].items():\nids_ = self._alias_to_ids(alias)\nfor node_id in ids_:\nif node_id not in visited:\nnode = self.get_node_by_id(node_id)\nnode_prio_flag = _override_node_config(node, conf_node)\nprio_flag = node_prio_flag or prio_flag  # keep track of flag\nelse:\nraise ValueError(\nf\"trying to set two configs for node {node_id}.\\n 1) {visited[node_id]}\\n 2) {conf_node}\"\n)\nvisited[node_id] = conf_node\nif \"max_concurrency\" in config:\nself.max_concurrency = config[\"max_concurrency\"]\nif prio_flag:\n# if we changed the priority of some nodes we need to recompute the compound prio\nself._assign_compound_priority()\n</code></pre>"},{"location":"dag/#tawazi._dag.dag.DAG.config_from_json","title":"<code>config_from_json(config_path)</code>","text":"<p>Allows reconfiguring the parameters of the nodes from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>the path to the JSON file</p> required Source code in <code>tawazi/_dag/dag.py</code> <pre><code>def config_from_json(self, config_path: str) -&gt; None:\n\"\"\"Allows reconfiguring the parameters of the nodes from a JSON file.\n    Args:\n        config_path: the path to the JSON file\n    \"\"\"\nwith open(config_path) as f:\njson_config = json.load(f)\nself.config_from_dict(json_config)\n</code></pre>"},{"location":"dag/#tawazi._dag.dag.DAG.config_from_yaml","title":"<code>config_from_yaml(config_path)</code>","text":"<p>Allows reconfiguring the parameters of the nodes from a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>the path to the YAML file</p> required Source code in <code>tawazi/_dag/dag.py</code> <pre><code>def config_from_yaml(self, config_path: str) -&gt; None:\n\"\"\"Allows reconfiguring the parameters of the nodes from a YAML file.\n    Args:\n        config_path: the path to the YAML file\n    \"\"\"\nwith open(config_path) as f:\nyaml_config = yaml.load(f, Loader=_UniqueKeyLoader)  # noqa: S506\nself.config_from_dict(yaml_config)\n</code></pre>"},{"location":"dag/#tawazi._dag.dag.DAG.draw","title":"<code>draw(k=0.8, display=True, t=3)</code>","text":"<p>Draws the Networkx directed graph.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>float</code> <p>parameter for the layout of the graph, the higher, the further the nodes apart. Defaults to 0.8.</p> <code>0.8</code> <code>display</code> <code>bool</code> <p>display the layout created. Defaults to True.</p> <code>True</code> <code>t</code> <code>int</code> <p>time to display in seconds. Defaults to 3.</p> <code>3</code> Source code in <code>tawazi/_dag/dag.py</code> <pre><code>def draw(self, k: float = 0.8, display: bool = True, t: int = 3) -&gt; None:\n\"\"\"Draws the Networkx directed graph.\n    Args:\n        k (float): parameter for the layout of the graph, the higher, the further the nodes apart. Defaults to 0.8.\n        display (bool): display the layout created. Defaults to True.\n        t (int): time to display in seconds. Defaults to 3.\n    \"\"\"\nimport matplotlib.pyplot as plt\n# TODO: use graphviz instead! it is much more elegant\npos = nx.spring_layout(self.graph_ids, seed=42069, k=k, iterations=20)\nnx.draw(self.graph_ids, pos, with_labels=True)\nif display:\nplt.ion()\nplt.show()\ntime.sleep(t)\nplt.close()\n</code></pre>"},{"location":"dag/#tawazi._dag.dag.DAG.executor","title":"<code>executor(**kwargs)</code>","text":"<p>Generates a DAGExecution for the DAG.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to be passed to DAGExecution's constructor</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>DAGExecution</code> <code>DAGExecution[P, RVDAG]</code> <p>an executor for the DAG</p> Source code in <code>tawazi/_dag/dag.py</code> <pre><code>def executor(self, **kwargs: Any) -&gt; \"DAGExecution[P, RVDAG]\":\n\"\"\"Generates a DAGExecution for the DAG.\n    Args:\n        **kwargs (Any): keyword arguments to be passed to DAGExecution's constructor\n    Returns:\n        DAGExecution: an executor for the DAG\n    \"\"\"\nreturn DAGExecution(self, **kwargs)\n</code></pre>"},{"location":"dag/#tawazi._dag.dag.DAG.get_node_by_id","title":"<code>get_node_by_id(id_)</code>","text":"<p>Get the ExecNode with the given id.</p> the returned ExecNode is not modified by any execution! <p>This means that you can not get the result of its execution via <code>DAG.get_node_by_id(&lt;id&gt;).result</code>. In order to do that, you need to make a DAGExecution and then call <code>DAGExecution.get_node_by_id(&lt;id&gt;).result</code>, which will contain the results.</p> <p>Parameters:</p> Name Type Description Default <code>id_</code> <code>Identifier</code> <p>id of the ExecNode</p> required <p>Returns:</p> Name Type Description <code>ExecNode</code> <code>ExecNode</code> <p>corresponding ExecNode</p> Source code in <code>tawazi/_dag/dag.py</code> <pre><code>def get_node_by_id(self, id_: Identifier) -&gt; ExecNode:\n\"\"\"Get the ExecNode with the given id.\n    Note: the returned ExecNode is not modified by any execution!\n        This means that you can not get the result of its execution via `DAG.get_node_by_id(&lt;id&gt;).result`.\n        In order to do that, you need to make a DAGExecution and then call `DAGExecution.get_node_by_id(&lt;id&gt;).result`, which will contain the results.\n    Args:\n        id_ (Identifier): id of the ExecNode\n    Returns:\n        ExecNode: corresponding ExecNode\n    \"\"\"\n# TODO: ? catch the keyError and\n#   help the user know the id of the ExecNode by pointing to documentation!?\nreturn self.node_dict[id_]\n</code></pre>"},{"location":"dag/#tawazi._dag.dag.DAG.get_nodes_by_tag","title":"<code>get_nodes_by_tag(tag)</code>","text":"<p>Get the ExecNodes with the given tag.</p> the returned ExecNode is not modified by any execution! <p>This means that you can not get the result of its execution via <code>DAG.get_nodes_by_tag(&lt;tag&gt;).result</code>. In order to do that, you need to make a DAGExecution and then call <code>DAGExecution.get_nodes_by_tag(&lt;tag&gt;).result</code>, which will contain the results.</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>Any</code> <p>tag of the ExecNodes</p> required <p>Returns:</p> Type Description <code>List[ExecNode]</code> <p>List[ExecNode]: corresponding ExecNodes</p> Source code in <code>tawazi/_dag/dag.py</code> <pre><code>def get_nodes_by_tag(self, tag: Tag) -&gt; List[ExecNode]:\n\"\"\"Get the ExecNodes with the given tag.\n    Note: the returned ExecNode is not modified by any execution!\n        This means that you can not get the result of its execution via `DAG.get_nodes_by_tag(&lt;tag&gt;).result`.\n        In order to do that, you need to make a DAGExecution and then call `DAGExecution.get_nodes_by_tag(&lt;tag&gt;).result`, which will contain the results.\n    Args:\n        tag (Any): tag of the ExecNodes\n    Returns:\n        List[ExecNode]: corresponding ExecNodes\n    \"\"\"\nif isinstance(tag, Tag):\nreturn self.tagged_nodes[tag]\nraise TypeError(f\"tag {tag} must be of Tag type. Got {type(tag)}\")\n</code></pre>"},{"location":"dag/#tawazi._dag.dag.DAG.setup","title":"<code>setup(target_nodes=None, exclude_nodes=None)</code>","text":"<p>Run the setup ExecNodes for the DAG.</p> <p>If target_nodes are provided, run only the necessary setup ExecNodes, otherwise will run all setup ExecNodes.</p> <code>DAG</code> arguments should not be passed to setup ExecNodes. <p>Only pass in constants or setup <code>ExecNode</code>s results.</p> <p>Parameters:</p> Name Type Description Default <code>target_nodes</code> <code>Optional[List[XNId]]</code> <p>The ExecNodes that the user aims to use in the DAG. This might include setup or non setup ExecNodes. If None is provided, will run all setup ExecNodes. Defaults to None.</p> <code>None</code> <code>exclude_nodes</code> <code>Optional[List[XNId]]</code> <p>The ExecNodes that the user aims to exclude from the DAG. The user is responsible for ensuring that the overlapping between the target_nodes and exclude_nodes is logical.</p> <code>None</code> Source code in <code>tawazi/_dag/dag.py</code> <pre><code>def setup(\nself,\ntarget_nodes: Optional[Sequence[Alias]] = None,\nexclude_nodes: Optional[Sequence[Alias]] = None,\n) -&gt; None:\n\"\"\"Run the setup ExecNodes for the DAG.\n    If target_nodes are provided, run only the necessary setup ExecNodes, otherwise will run all setup ExecNodes.\n    NOTE: `DAG` arguments should not be passed to setup ExecNodes.\n        Only pass in constants or setup `ExecNode`s results.\n    Args:\n        target_nodes (Optional[List[XNId]], optional): The ExecNodes that the user aims to use in the DAG.\n            This might include setup or non setup ExecNodes. If None is provided, will run all setup ExecNodes. Defaults to None.\n        exclude_nodes (Optional[List[XNId]], optional): The ExecNodes that the user aims to exclude from the DAG.\n            The user is responsible for ensuring that the overlapping between the target_nodes and exclude_nodes is logical.\n    \"\"\"\n# 1. select all setup ExecNodes\n#  do not copy the setup nodes because we want them to be modified per DAG instance!\nall_setup_nodes = {\nid_: xn\nfor id_, xn in self.node_dict.items()\nif xn.setup or (isinstance(xn, ArgExecNode) and xn.executed)\n}\n# 2. if target_nodes is not provided run all setup ExecNodes\nif target_nodes is None:\ntarget_ids = list(all_setup_nodes.keys())\ngraph = self._make_subgraph(target_ids, exclude_nodes)\nelse:\n# 2.1 the leaves_ids that the user wants to execute\n#  however they might contain non setup nodes... so we should extract all the nodes ids\n#  that must be run in order to run the target_nodes ExecNodes\n#  afterwards we can remove the non setup nodes\ntarget_ids = self._get_target_ids(target_nodes)\n# 2.2 filter non setup ExecNodes\ngraph = self._make_subgraph(target_ids, exclude_nodes)\nids_to_remove = [id_ for id_ in graph if id_ not in all_setup_nodes]\nfor id_ in ids_to_remove:\ngraph.remove_node(id_)\n# TODO: handle debug XNs!\nself._execute(graph, all_setup_nodes)\n</code></pre>"},{"location":"dag/#tawazi._dag.dag.DAG.compose","title":"<code>compose(inputs, outputs, **kwargs)</code>","text":"<p>Compose a new DAG using inputs and outputs ExecNodes (Experimental).</p> <p>All provided <code>Alias</code>es must point to unique <code>ExecNode</code>s. Otherwise ValueError is raised The user is responsible to correctly specify inputs and outputs signature of the <code>DAG</code>. * The inputs can be specified as a single <code>Alias</code> or a <code>Sequence</code> of <code>Alias</code>es. * The outputs can be specified as a single <code>Alias</code> (a single value is returned) or a <code>Sequence</code> of <code>Alias</code>es in which case a Tuple of the values are returned. If outputs are specified as [], () is returned. The syntax is the following:</p> <p>from tawazi import dag, xn, DAG from typing import Tuple, Any @xn ... def unwanted_xn() -&gt; int: return 42 @xn ... def x(v: Any) -&gt; int: return int(v) @xn ... def y(v: Any) -&gt; str: return str(v) @xn ... def z(x: int, y: str) -&gt; float: return float(x) + float(y) @dag ... def pipe() -&gt; Tuple[int, float, int]: ...     a = unwanted_xn() ...     res = z(x(1), y(1)) ...     b = unwanted_xn() ...     return a, res, b composed_dag = pipe.compose([x, y], z) assert composed_dag(1, 1) == 2.0</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Alias | List[Alias]</code> <p>the Inputs nodes whose results are provided.</p> required <code>outputs</code> <code>Alias | List[Alias]</code> <p>the Output nodes that must execute last, The ones that will generate results</p> required <code>**kwargs</code> <code>Dict[str, Any]</code> <p>additional arguments to be passed to the DAG's constructor</p> <code>{}</code> Source code in <code>tawazi/_dag/dag.py</code> <pre><code>def compose(self, inputs: Union[Alias, Sequence[Alias]], outputs: Union[Alias, Sequence[Alias]], **kwargs: Dict[str, Any]) -&gt; \"DAG\":  # type: ignore[type-arg]\n\"\"\"Compose a new DAG using inputs and outputs ExecNodes (Experimental).\n    All provided `Alias`es must point to unique `ExecNode`s. Otherwise ValueError is raised\n    The user is responsible to correctly specify inputs and outputs signature of the `DAG`.\n    * The inputs can be specified as a single `Alias` or a `Sequence` of `Alias`es.\n    * The outputs can be specified as a single `Alias` (a single value is returned)\n    or a `Sequence` of `Alias`es in which case a Tuple of the values are returned.\n    If outputs are specified as [], () is returned.\n    The syntax is the following:\n    &gt;&gt;&gt; from tawazi import dag, xn, DAG\n    &gt;&gt;&gt; from typing import Tuple, Any\n    &gt;&gt;&gt; @xn\n    ... def unwanted_xn() -&gt; int: return 42\n    &gt;&gt;&gt; @xn\n    ... def x(v: Any) -&gt; int: return int(v)\n    &gt;&gt;&gt; @xn\n    ... def y(v: Any) -&gt; str: return str(v)\n    &gt;&gt;&gt; @xn\n    ... def z(x: int, y: str) -&gt; float: return float(x) + float(y)\n    &gt;&gt;&gt; @dag\n    ... def pipe() -&gt; Tuple[int, float, int]:\n    ...     a = unwanted_xn()\n    ...     res = z(x(1), y(1))\n    ...     b = unwanted_xn()\n    ...     return a, res, b\n    &gt;&gt;&gt; composed_dag = pipe.compose([x, y], z)\n    &gt;&gt;&gt; assert composed_dag(1, 1) == 2.0\n    &gt;&gt;&gt; # composed_dag: DAG[[int, str], float] = pipe.compose([x, y], [z])  # optional typing of the returned DAG!\n    &gt;&gt;&gt; # assert composed_dag(1, 1) == 2.0  # type checked!\n    Args:\n        inputs (Alias | List[Alias]): the Inputs nodes whose results are provided.\n        outputs (Alias | List[Alias]): the Output nodes that must execute last, The ones that will generate results\n        **kwargs (Dict[str, Any]): additional arguments to be passed to the DAG's constructor\n    \"\"\"\n# what happens for edge cases ??\n# 1. if inputs are less than sufficient to produce outputs (-&gt; error)\n# 2. if inputs are more than sufficient to produce outputs (-&gt; warning)\n# 3. if inputs are successors of outputs (-&gt; error)\n# 5. if inputs are successors of inputs but predecessors of outputs\n# 1. inputs and outputs are overlapping (-&gt; error ambiguous ? maybe not)\n# 1. if inputs &amp; outputs are [] (-&gt; ())\n# 4. cst cubgraph inputs is [], outputs is not [] but contains constants (-&gt; works as expected)\n# 5. inputs is not [], outputs is [] same as 2.\n# 6. a subcase of the above (some inputs suffice to produce some of the outputs, but some inputs don't)\n# 7. advanced usage: if inputs contain ... (i.e. Ellipsis) in this case we must expand it to reach all the remaining XN in a smart manner\n#  we should keep the order of the inputs and outputs (future)\n# 8. what if some arguments have default values? should they be provided by the user?\n# 9. how to specify that arguments of the original DAG should be provided by the user? the user should provide the input's ID which is not a stable Alias yet\ndef _alias_or_aliases_to_ids(\nalias_or_aliases: Union[Alias, Sequence[Alias]]\n) -&gt; List[Identifier]:\nif isinstance(alias_or_aliases, str) or isinstance(alias_or_aliases, ExecNode):\nreturn [self._get_single_xn_by_alias(alias_or_aliases).id]\nreturn [self._get_single_xn_by_alias(a_id).id for a_id in alias_or_aliases]\ndef _raise_input_successor_of_input(pred: Identifier, succ: Set[Identifier]) -&gt; NoReturn:\nraise ValueError(\nf\"Input ExecNodes {succ} depend on Input ExecNode {pred}.\"\nf\"this is ambiguous. Remove either one of them.\"\n)\ndef _raise_missing_input(input_: Identifier) -&gt; NoReturn:\nraise ValueError(\nf\"ExecNode {input_} are not declared as inputs. \"\nf\"Either declare them as inputs or modify the requests outputs.\"\n)\ndef _alias_or_aliases_to_uxns(\nalias_or_aliases: Union[Alias, Sequence[Alias]]\n) -&gt; ReturnUXNsType:\nif isinstance(alias_or_aliases, str) or isinstance(alias_or_aliases, ExecNode):\nreturn UsageExecNode(self._get_single_xn_by_alias(alias_or_aliases).id)\nreturn tuple(\nUsageExecNode(self._get_single_xn_by_alias(a_id).id) for a_id in alias_or_aliases\n)\n# 1. get input ids and output ids.\n#  Alias should correspond to a single ExecNode,\n#  otherwise an ambiguous situation exists, raise error\nin_ids = _alias_or_aliases_to_ids(inputs)\nout_ids = _alias_or_aliases_to_ids(outputs)\n# 2.1 contains all the ids of the nodes that will be in the new DAG\nset_xn_ids = set(in_ids + out_ids)\n# 2.2 all ancestors of the inputs\nin_ids_ancestors: Set[Identifier] = self.graph_ids.ancestors_of_iter(in_ids)\n# 3. check edge cases\n# inputs should not be successors of inputs, otherwise (error)\n# and inputs should produce at least one of the outputs, otherwise (warning)\nfor i in in_ids:\n# if pred is ancestor of an input, raise error\nif i in in_ids_ancestors:\n_raise_input_successor_of_input(i, set_xn_ids)\n# if i doesn't produce any of the wanted outputs, raise a warning!\ndescendants: Set[Identifier] = nx.descendants(self.graph_ids, i)\nif descendants.isdisjoint(out_ids):\nwarnings.warn(\nf\"Input ExecNode {i} is not used to produce any of the requested outputs.\"\nf\"Consider removing it from the inputs.\",\nstacklevel=2,\n)\n# 4. collect necessary ExecNodes' IDS\n# 4.1 original DAG's inputs that don't contain default values.\n# used to detect missing inputs\ndag_inputs_ids = [\nuxn.id for uxn in self.input_uxns if self.node_dict[uxn.id].result is NoVal\n]\n# 4.2 define helper function\ndef _add_missing_deps(candidate_id: Identifier, set_xn_ids: Set[Identifier]) -&gt; None:\n\"\"\"Adds missing dependency to the set of ExecNodes that will be in the new DAG.\n        Note: uses nonlocal variable dag_inputs_ids\n        Args:\n            candidate_id (Identifier): candidate id of an `ExecNode` that will be in the new DAG\n            set_xn_ids (Set[Identifier]): Set of `ExecNode`s that will be in the new DAG\n        \"\"\"\npreds = self.graph_ids.predecessors(candidate_id)\nfor pred in preds:\nif pred not in set_xn_ids:\n# this candidate is necessary to produce the output,\n# it is an input to the original DAG\n# it is not provided as an input to the composed DAG\n# hence the user forgot to supply it! (raise error)\nif pred in dag_inputs_ids:\n_raise_missing_input(pred)\n# necessary intermediate dependency.\n# collect it in the set\nset_xn_ids.add(pred)\n_add_missing_deps(pred, set_xn_ids)\n# 4.3 add all required dependencies for each output\nfor o_id in out_ids:\n_add_missing_deps(o_id, set_xn_ids)\n# 5.1 copy the ExecNodes that will be in the composed DAG because\n#  maybe the composed DAG will modify them (e.g. change their tags)\n#  and we don't want to modify the original DAG\nxn_dict = {xn_id: copy(self.node_dict[xn_id]) for xn_id in set_xn_ids}\n# 5.2 change the inputs of the ExecNodes into ArgExecNodes\nfor xn_id, xn in xn_dict.items():\nif xn_id in in_ids:\nlogger.debug(\"changing Composed-DAG's input {} into ArgExecNode\", xn_id)\nxn.__class__ = ArgExecNode\nxn.exec_function = _make_raise_arg_error(\"composed\", xn.id)\n# eliminate all dependencies\nxn.args = []\nxn.kwargs = {}\n# 5.3 make the inputs and outputs UXNs for the composed DAG\nin_uxns = [UsageExecNode(xn_id) for xn_id in in_ids]\n# if a single value is returned make the output a single value\nout_uxns = _alias_or_aliases_to_uxns(outputs)\n# 6. return the composed DAG\n# ignore[arg-type] because the type of the kwargs is not known\nreturn DAG(xn_dict, in_uxns, out_uxns, **kwargs)  # type: ignore[arg-type]\n</code></pre>"},{"location":"dag/#tawazi._dag.dag.DAG.compose--composed_dag-dagint-str-float-pipecomposex-y-z-optional-typing-of-the-returned-dag","title":"composed_dag: DAG[[int, str], float] = pipe.compose([x, y], [z])  # optional typing of the returned DAG!","text":""},{"location":"dag/#tawazi._dag.dag.DAG.compose--assert-composed_dag1-1-20-type-checked","title":"assert composed_dag(1, 1) == 2.0  # type checked!","text":""},{"location":"decorators/","title":"decorators","text":"<p>Decorators of Tawazi.</p> <p>The user should use the decorators <code>@dag</code> and <code>@xn</code> to create Tawazi objects <code>DAG</code> and <code>ExecNode</code>.</p>"},{"location":"decorators/#tawazi._decorators.xn","title":"<code>xn(func=None, *, priority=0, is_sequential=cfg.TAWAZI_IS_SEQUENTIAL, debug=False, tag=None, setup=False, unpack_to=None, resource=cfg.TAWAZI_DEFAULT_RESOURCE)</code>","text":"<p>Decorate a normal function to make it an ExecNode.</p> <p>When the decorated function is called inside a <code>DAG</code>, you are actually calling an <code>ExecNode</code>. This way we can record the dependencies in order to build the actual DAG. Please check the example in the README for a guide to the usage.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>[Callable[P, RVXN]</code> <p>a Callable that will be executed in the <code>DAG</code></p> <code>None</code> <code>priority</code> <code>int</code> <p>priority of the execution with respect to other <code>ExecNode</code>s</p> <code>0</code> <code>is_sequential</code> <code>bool</code> <p>whether to allow the execution of this <code>ExecNode</code> with others or not. If <code>True</code>, all other <code>ExecNode</code> currently running will stop before this one starts executing.</p> <code>cfg.TAWAZI_IS_SEQUENTIAL</code> <code>debug</code> <code>bool</code> <p>if <code>True</code>, will execute only when Debug mode is active. a debug <code>ExecNode</code> will run its inputs exists regardless of subgraph choice.</p> <code>False</code> <code>tag</code> <code>Optional[TagOrTags]</code> <p>a str or Tuple[str] to tag this ExecNode. If Tuple[str] is given, every value of the tuple is used as tag. Notice that multiple ExecNodes can have the same tag.</p> <code>None</code> <code>setup</code> <code>bool</code> <p>if True, will be executed only once during the lifetime of a <code>DAG</code> instance. Setup <code>ExecNode</code>s are meant to be used to load heavy data only once inside the execution pipeline and then be used as if the results of their execution were cached. This can be useful if you want to load heavy ML models, heavy Data etc. Note that you can run all / subset of the setup nodes by invoking the DAG.setup method NOTE setup nodes are currently not threadsafe!     because they are shared between all threads!     If you execute the same pipeline in multiple threads during the setup phase, the behavior is undefined.     This is why it is best to invoke the DAG.setup method before using the DAG in a multithreaded environment.     This problem will be resolved in the future</p> <code>False</code> <code>unpack_to</code> <code>Optional[int]</code> <p>if not None, this ExecNode's execution must return unpacked results corresponding to the given value</p> <code>None</code> <code>resource</code> <code>str</code> <p>the resource to use to execute this ExecNode. Defaults to \"thread\".</p> <code>cfg.TAWAZI_DEFAULT_RESOURCE</code> <p>Returns:</p> Name Type Description <code>LazyExecNode</code> <code>Union[Callable[[Callable[P, RVXN]], LazyExecNode[P, RVXN]], LazyExecNode[P, RVXN]]</code> <p>The decorated function wrapped in an <code>ExecNode</code>.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the decorated function passed is not a <code>Callable</code>.</p> Source code in <code>tawazi/_decorators.py</code> <pre><code>def xn(\nfunc: Optional[Callable[P, RVXN]] = None,\n*,\npriority: int = 0,\nis_sequential: bool = cfg.TAWAZI_IS_SEQUENTIAL,\ndebug: bool = False,\ntag: Optional[TagOrTags] = None,\nsetup: bool = False,\nunpack_to: Optional[int] = None,\nresource: Resource = cfg.TAWAZI_DEFAULT_RESOURCE,\n) -&gt; Union[Callable[[Callable[P, RVXN]], LazyExecNode[P, RVXN]], LazyExecNode[P, RVXN]]:\n\"\"\"Decorate a normal function to make it an ExecNode.\n    When the decorated function is called inside a `DAG`, you are actually calling an `ExecNode`.\n    This way we can record the dependencies in order to build the actual DAG.\n    Please check the example in the README for a guide to the usage.\n    Args:\n        func ([Callable[P, RVXN]): a Callable that will be executed in the `DAG`\n        priority (int): priority of the execution with respect to other `ExecNode`s\n        is_sequential (bool): whether to allow the execution of this `ExecNode` with others or not.\n            If `True`, all other `ExecNode` currently running will stop before this one starts executing.\n        debug (bool): if `True`, will execute only when Debug mode is active.\n            a debug `ExecNode` will run its inputs exists regardless of subgraph choice.\n        tag (Optional[TagOrTags]): a str or Tuple[str] to tag this ExecNode.\n            If Tuple[str] is given, every value of the tuple is used as tag.\n            Notice that multiple ExecNodes can have the same tag.\n        setup (bool): if True, will be executed only once during the lifetime of a `DAG` instance.\n            Setup `ExecNode`s are meant to be used to load heavy data only once inside the execution pipeline\n            and then be used as if the results of their execution were cached.\n            This can be useful if you want to load heavy ML models, heavy Data etc.\n            Note that you can run all / subset of the setup nodes by invoking the DAG.setup method\n            NOTE setup nodes are currently not threadsafe!\n                because they are shared between all threads!\n                If you execute the same pipeline in multiple threads during the setup phase, the behavior is undefined.\n                This is why it is best to invoke the DAG.setup method before using the DAG in a multithreaded environment.\n                This problem will be resolved in the future\n        unpack_to (Optional[int]): if not None, this ExecNode's execution must return unpacked results corresponding to the given value\n        resource (str): the resource to use to execute this ExecNode. Defaults to \"thread\".\n    Returns:\n        LazyExecNode: The decorated function wrapped in an `ExecNode`.\n    Raises:\n        TypeError: If the decorated function passed is not a `Callable`.\n    \"\"\"\ndef intermediate_wrapper(_func: Callable[P, RVXN]) -&gt; LazyExecNode[P, RVXN]:\nlazy_exec_node = LazyExecNode(\n_func, priority, is_sequential, debug, tag, setup, unpack_to, resource\n)\nfunctools.update_wrapper(lazy_exec_node, _func)\nreturn lazy_exec_node\n# case #1: arguments are provided to the decorator\nif func is None:\nreturn intermediate_wrapper\n# case #2: no argument is provided to the decorator\nif not callable(func):\nraise TypeError(f\"{func} is not a callable. Did you use a non-keyword argument?\")\nreturn intermediate_wrapper(func)\n</code></pre>"},{"location":"decorators/#tawazi._decorators.dag","title":"<code>dag(declare_dag_function=None, *, max_concurrency=1, behavior=ErrorStrategy.strict)</code>","text":"<p>Transform the declared <code>ExecNode</code>s into a DAG that can be executed by Tawazi's scheduler.</p> <p>The same DAG can be executed multiple times.</p> dag is thread safe because it uses an internal lock. <p>If you need to construct lots of DAGs in multiple threads, it is best to construct your dag once and then use it as much as you like.</p> <p>Please check the example in the README for a guide to the usage.</p> <p>Parameters:</p> Name Type Description Default <code>declare_dag_function</code> <code>Optional[Callable[P, RVDAG]]</code> <p>a function that describes the execution of the DAG. This function should only contain calls to <code>ExecNode</code>s and data Exchange between them. (i.e. You can not use a normal Python function inside it unless decorated with <code>@xn</code>.) However, you can use some simple python code to generate constants. These constants are computed only once during the <code>DAG</code> declaration.</p> <code>None</code> <code>max_concurrency</code> <code>int</code> <p>the maximum number of concurrent threads to execute in parallel.</p> <code>1</code> <code>behavior</code> <code>ErrorStrategy</code> <p>the behavior of the <code>DAG</code> when an error occurs during the execution of a function (<code>ExecNode</code>).</p> <code>ErrorStrategy.strict</code> <p>Returns:</p> Type Description <code>Union[Callable[[Callable[P, RVDAG]], DAG[P, RVDAG]], DAG[P, RVDAG]]</code> <p>a <code>DAG</code> instance that can be used just like a normal Python function. However it will be executed by Tawazi's scheduler.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the decorated object is not a Callable.</p> Source code in <code>tawazi/_decorators.py</code> <pre><code>def dag(\ndeclare_dag_function: Optional[Callable[P, RVDAG]] = None,\n*,\nmax_concurrency: int = 1,\nbehavior: ErrorStrategy = ErrorStrategy.strict,\n) -&gt; Union[Callable[[Callable[P, RVDAG]], DAG[P, RVDAG]], DAG[P, RVDAG]]:\n\"\"\"Transform the declared `ExecNode`s into a DAG that can be executed by Tawazi's scheduler.\n    The same DAG can be executed multiple times.\n    Note: dag is thread safe because it uses an internal lock.\n        If you need to construct lots of DAGs in multiple threads,\n        it is best to construct your dag once and then use it as much as you like.\n    Please check the example in the README for a guide to the usage.\n    Args:\n        declare_dag_function: a function that describes the execution of the DAG.\n            This function should only contain calls to `ExecNode`s and data Exchange between them.\n            (i.e. You can not use a normal Python function inside it unless decorated with `@xn`.)\n            However, you can use some simple python code to generate constants.\n            These constants are computed only once during the `DAG` declaration.\n        max_concurrency: the maximum number of concurrent threads to execute in parallel.\n        behavior: the behavior of the `DAG` when an error occurs during the execution of a function (`ExecNode`).\n    Returns:\n        a `DAG` instance that can be used just like a normal Python function. However it will be executed by Tawazi's scheduler.\n    Raises:\n        TypeError: If the decorated object is not a Callable.\n    \"\"\"\n# wrapper used to support parametrized and non parametrized decorators\ndef intermediate_wrapper(_func: Callable[P, RVDAG]) -&gt; DAG[P, RVDAG]:\n# 0. Protect against multiple threads declaring many DAGs at the same time\nwith node.exec_nodes_lock:\n# 1. node.exec_nodes contains all the ExecNodes that concern the DAG being built at the moment.\n#      make sure it is empty\nnode.exec_nodes = {}\ntry:\n# 2. make ExecNodes corresponding to the arguments of the ExecNode\n# 2.1 get the names of the arguments and the default values\nfunc_args, func_default_args = get_args_and_default_args(_func)\n# 2.2 Construct non default arguments.\n# Corresponding values must be provided during usage\nargs: List[ExecNode] = [ArgExecNode(_func, arg_name) for arg_name in func_args]\n# 2.2 Construct Default arguments.\nargs.extend(\n[\nArgExecNode(_func, arg_name, arg)\nfor arg_name, arg in func_default_args.items()\n]\n)\n# 2.3 Arguments are also ExecNodes that get executed inside the scheduler\nnode.exec_nodes.update({xn.id: xn for xn in args})\n# 2.4 make UsageExecNodes for input arguments\nuxn_args = [UsageExecNode(xn.id) for xn in args]\n# 3. Execute the dependency describer function\n# NOTE: Only ordered parameters are supported at the moment!\n#  No **kwargs!! Only positional Arguments\n# used to be fetch the results at the end of the computation\nreturned_val: Any = _func(*uxn_args)  # type: ignore[arg-type]\nreturned_usage_exec_nodes: ReturnUXNsType = wrap_in_uxns(_func, returned_val)\n# 4. Construct the DAG instance\nd: DAG[P, RVDAG] = DAG(\nnode.exec_nodes,\ninput_uxns=uxn_args,\nreturn_uxns=returned_usage_exec_nodes,\nmax_concurrency=max_concurrency,\nbehavior=behavior,\n)\n# clean up even in case an error is raised during dag construction\nfinally:\n# 5. Clean global variable\n# node.exec_nodes are deep copied inside the DAG.\n#   we can empty the global variable node.exec_nodes\nnode.exec_nodes = {}\nfunctools.update_wrapper(d, _func)\nreturn d\n# case 1: arguments are provided to the decorator\nif declare_dag_function is None:\n# return a decorator\nreturn intermediate_wrapper\n# case 2: arguments aren't provided to the decorator\nif not callable(declare_dag_function):\nraise TypeError(\nf\"{declare_dag_function} is not a callable. Did you use a non-keyword argument?\"\n)\nreturn intermediate_wrapper(declare_dag_function)\n</code></pre>"},{"location":"future_developments/","title":"Future Developments","text":""},{"location":"future_developments/#soon-to-be-released","title":"Soon to be released","text":"<p>A couple of features will be released soon:</p> <ul> <li>handle problems when calling <code>ExecNodes</code> wrongly.</li> <li>(for example when using args as parameters but only *kwargs are provided).</li> <li>Calling <code>ExecNodes</code> must be similar to calling the original function (must imitate the same signature otherwise raise the correct exception).</li> <li>support mixing ExecNodes and non <code>ExecNodes</code> functions.</li> <li>test the case where <code>ExecNodes</code> are stored in a list and then passed via * operator.</li> <li>an example of running a set of calculations with the <code>DAG</code> and without the <code>DAG</code>.</li> <li>we can show that using tawazi is transparent and one just has to remove the decorators. Then everything is back to normal.</li> <li>decide whether to identify the <code>ExecNode</code> by a Hashable ID or by its own Python ID. This is breaking change and must change to 0.2.1.</li> <li>support constants by resolving the error in the tests.</li> <li>Remove the global object and maybe replace it with an attribute to the creating function.</li> <li>improve the graph dependency rendering on the console (using graphviz).</li> <li>automatically generate release on new tag https://docs.github.com/en/repositories/releasing-projects-on-github/automatically-generated-release-notes#configuring-automatically-generated-release-notes</li> <li>use opnssf service to evaluate code best practices https://bestpractices.coreinfrastructure.org/fr/projects/1486</li> </ul>"},{"location":"future_developments/#features-to-be-discussed","title":"Features to be discussed","text":"<ul> <li>support multiprocessing.</li> <li>simulation of the execution using a <code>DAG</code> stored ledger.</li> <li>Disallow execution in parallel of some threads in parallel with some other threads.</li> <li>maybe by making a group of threads that are CPU bound and a group of threads that are IO bound ?</li> <li>Remove dependency on networkx to make <code>tawazi</code> a standalone package.</li> <li>save the results of the calculation in pickled format in case an error is encountered ? or just at the end of the run</li> <li>re-run the same calculations of the graph but take the input from the presaved pickle files instead</li> <li>put documentation about different cases where it is advantageous to use it</li> <li>in methods not only in functions</li> <li>in a gunicorn application</li> <li>for getting information from multiple resources</li> <li>pretty-print the graph deps on the console:</li> </ul> <ul> <li>remove the argument_name behavior using a more intelligent way</li> <li>or instead of doing this, you can make this optional and then prefer to infer the argument_name more intelligently using the place in the arguments where the ExecNode was passed!</li> </ul>"},{"location":"nodes/","title":"ExecNodes","text":"<p>warning: This page describes some internal functionalities of Tawazi, it is still subject to change during minor releases.</p> <p>This class is the base executable node of the Directed Acyclic Execution Graph.</p> <p>An ExecNode is an Object that can be executed inside a DAG scheduler.</p> <p>It basically consists of a function (exec_function) that takes args and kwargs and returns a value.</p> <p>When the ExecNode is executed in the DAG, the resulting value will be stored in the ExecNode.result instance attribute.</p> This class is not meant to be instantiated directly. <p>Please use <code>@xn</code> decorator.</p> Source code in <code>tawazi/node/node.py</code> <pre><code>class ExecNode:\n\"\"\"This class is the base executable node of the Directed Acyclic Execution Graph.\n    An ExecNode is an Object that can be executed inside a DAG scheduler.\n    It basically consists of a function (exec_function) that takes args and kwargs and returns a value.\n    When the ExecNode is executed in the DAG, the resulting value will be stored in the ExecNode.result instance attribute.\n    Note: This class is not meant to be instantiated directly.\n        Please use `@xn` decorator.\n    \"\"\"\ndef __init__(\nself,\nid_: Identifier,\nexec_function: Callable[..., Any] = lambda *args, **kwargs: None,\nargs: Optional[List[\"UsageExecNode\"]] = None,\nkwargs: Optional[Dict[str, \"UsageExecNode\"]] = None,\npriority: int = 0,\nis_sequential: bool = cfg.TAWAZI_IS_SEQUENTIAL,\ndebug: bool = False,\ntag: Optional[TagOrTags] = None,\nsetup: bool = False,\nunpack_to: Optional[int] = None,\nresource: Resource = cfg.TAWAZI_DEFAULT_RESOURCE,\n):\n\"\"\"Constructor of ExecNode.\n        Args:\n            id_ (Identifier): identifier of ExecNode.\n            exec_function (Callable): a callable will be executed in the graph.\n                This is useful to make Joining ExecNodes (Nodes that enforce dependencies on the graph)\n            args (Optional[List[ExecNode]], optional): *args to pass to exec_function.\n            kwargs (Optional[Dict[str, ExecNode]], optional): **kwargs to pass to exec_function.\n            priority (int): priority compared to other ExecNodes; the higher the number the higher the priority.\n            is_sequential (bool): whether to execute this ExecNode in sequential order with respect to others.\n                When this ExecNode must be executed, all other nodes are waited to finish before starting execution.\n                Defaults to False.\n            debug (bool): Make this ExecNode a debug Node. Defaults to False.\n            tag (TagOrTags): Attach a Tag or Tags to this ExecNode. Defaults to None.\n            setup (bool): Make this ExecNode a setup Node. Defaults to False.\n            unpack_to (Optional[int]): if not None, this ExecNode's execution must return unpacked results corresponding to the given value\n            resource (str): the resource to use to execute this ExecNode. Defaults to \"thread\".\n        Raises:\n            ValueError: if setup and debug are both True.\n        \"\"\"\n# 1. assign attributes\nself._id = id_\nself.exec_function = exec_function\nself.priority = priority\nself.is_sequential = is_sequential\nself.debug = debug\nself.tag = tag\nself.setup = setup\nself.unpack_to = unpack_to\nself.active = True\nself.resource = resource\nself.args: List[UsageExecNode] = args or []\nself.kwargs: Dict[Identifier, UsageExecNode] = kwargs or {}\n# 2. compound_priority equals priority at the start but will be modified during the build process\nself.compound_priority = priority\n# 3. Assign the name\n# This can be used in the future but is not particularly useful at the moment\nself.__name__ = self.exec_function.__name__ if not isinstance(id_, str) else id_\n# 4. Assign a default NoVal to the result of the execution of this ExecNode,\n#  when this ExecNode will be executed, self.result will be overridden\n# It would be amazing if we can remove self.result and make ExecNode immutable\nself.result: Union[NoValType, Any] = NoVal\n\"\"\"Internal attribute to store the result of the execution of this ExecNode (Might change!).\"\"\"\n# even though setting result to NoVal is not necessary... it clarifies debugging\nself.profile = Profile(cfg.TAWAZI_PROFILE_ALL_NODES)\n@property\ndef executed(self) -&gt; bool:\n\"\"\"Whether this ExecNode has been executed.\"\"\"\nreturn self.result is not NoVal\ndef __repr__(self) -&gt; str:\n\"\"\"Human representation of the ExecNode.\n        Returns:\n            str: human representation of the ExecNode.\n        \"\"\"\nreturn f\"{self.__class__.__name__} {self.id} ~ | &lt;{hex(id(self))}&gt;\"\n# TODO: make cached_property ?\n@property\ndef dependencies(self) -&gt; List[\"UsageExecNode\"]:\n\"\"\"The List of ExecNode dependencies of This ExecNode.\n        Returns:\n            List[UsageExecNode]: the List of ExecNode dependencies of This ExecNode.\n        \"\"\"\n# Making the dependencies\n# 1. from args\ndeps = self.args.copy()\n# 2. and from kwargs\ndeps.extend(self.kwargs.values())\nreturn deps\n@property\ndef id(self) -&gt; Identifier:\n\"\"\"The identifier of this ExecNode.\"\"\"\nreturn self._id\n@property\ndef tag(self) -&gt; Optional[TagOrTags]:\n\"\"\"The Tag or Tags of this ExecNode.\"\"\"\nreturn self._tag\n@tag.setter\ndef tag(self, value: Optional[TagOrTags]) -&gt; None:\nis_none = value is None\nis_tag = isinstance(value, Tag)\nis_tuple_tag = isinstance(value, tuple) and all(isinstance(v, Tag) for v in value)\nif not (is_none or is_tag or is_tuple_tag):\nraise TypeError(\nf\"tag should be of type {TagOrTags} but {value} of type {type(value)} is provided\"\n)\nself._tag = value\n@property\ndef priority(self) -&gt; int:\n\"\"\"The priority of this ExecNode.\"\"\"\nreturn self._priority\n@priority.setter\ndef priority(self, value: int) -&gt; None:\nif not isinstance(value, int):\nraise ValueError(f\"priority must be an int, provided {type(value)}\")\nself._priority = value\n@property\ndef resource(self) -&gt; Resource:\n\"\"\"The resource used to run this ExecNode.\"\"\"\nreturn self._resource\n@resource.setter\ndef resource(self, value: Resource) -&gt; None:\nif not isinstance(value, Resource):\nraise ValueError(f\"resource must be of type {Resource}, provided {type(value)}\")\nself._resource = value\n@property\ndef is_sequential(self) -&gt; bool:\n\"\"\"Whether `ExecNode` runs in sequential order with respect to other `ExecNode`s.\"\"\"\nreturn self._is_sequential\n@is_sequential.setter\ndef is_sequential(self, value: bool) -&gt; None:\nif not isinstance(value, bool):\nraise TypeError(f\"is_sequential should be of type bool, but {value} provided\")\nself._is_sequential = value\n@property\ndef debug(self) -&gt; bool:\n\"\"\"Whether this ExecNode is a debug Node. ExecNode can't be setup and debug simultaneously.\"\"\"\nreturn self._debug\n@debug.setter\ndef debug(self, value: bool) -&gt; None:\nif not isinstance(value, bool):\nraise TypeError(f\"debug must be of type bool, but {value} provided\")\nself._debug = value\nself._validate()\n@property\ndef setup(self) -&gt; bool:\n\"\"\"Whether this ExecNode is a setup Node. ExecNode can't be setup and debug simultaneously.\"\"\"\nreturn self._setup\n@setup.setter\ndef setup(self, value: bool) -&gt; None:\nif not isinstance(value, bool):\nraise TypeError(f\"setup must be of type bool, but {value} provided\")\nself._setup = value\nself._validate()\n@property\ndef active(self) -&gt; Union[\"UsageExecNode\", bool]:\n\"\"\"Whether this ExecNode is active or not.\"\"\"\n# the value is set during the DAG description\nif isinstance(self._active, UsageExecNode):\nreturn self._active\n# the valule set is a constant value\nreturn bool(self._active)\n@active.setter\ndef active(self, value: Any) -&gt; None:\nself._active = value\ndef _execute(self, node_dict: Dict[Identifier, \"ExecNode\"]) -&gt; Optional[Any]:\n\"\"\"Execute the ExecNode inside of a DAG.\n        Args:\n            node_dict (Dict[Identifier, ExecNode]): A shared dictionary containing the other ExecNodes in the DAG;\n                the key is the id of the ExecNode. This node_dict refers to the current execution\n        Returns:\n            the result of the execution of the current ExecNode\n        \"\"\"\nlogger.debug(f\"Start executing {self.id} with task {self.exec_function}\")\nself.profile = Profile(cfg.TAWAZI_PROFILE_ALL_NODES)\nif self.executed:\nlogger.debug(f\"Skipping execution of a pre-computed node {self.id}\")\nreturn self.result\n# 1. prepare args and kwargs for usage:\nargs = [xnw.result(node_dict) for xnw in self.args]\nkwargs = {\nkey: xnw.result(node_dict)\nfor key, xnw in self.kwargs.items()\nif key not in RESERVED_KWARGS\n}\n# args = [arg.result for arg in self.args]\n# kwargs = {key: arg.result for key, arg in self.kwargs.items()}\n# 1. pre-\n# 1.1 prepare the profiling\nwith self.profile:\n# 2 post-\n# 2.1 write the result\nself.result = self.exec_function(*args, **kwargs)\n# 3. useless return value\nlogger.debug(f\"Finished executing {self.id} with task {self.exec_function}\")\nreturn self.result\ndef _validate(self) -&gt; None:\nif getattr(self, \"debug\", None) and getattr(self, \"setup\", None):\nraise ValueError(\nf\"The node {self.id} can't be a setup and a debug node at the same time.\"\n)\n@property\ndef unpack_to(self) -&gt; Optional[int]:\n\"\"\"The number of elements in the unpacked results of this ExecNode.\n        Returns:\n            Optional[int]: the number of elements in the unpacked results of this ExecNode.\n        \"\"\"\nreturn self._unpack_to\n@unpack_to.setter\ndef unpack_to(self, value: Optional[int]) -&gt; None:\nif value is not None:\nif not isinstance(value, int):\nraise ValueError(\nf\"unpack_to must be a positive int or None, provided {type(value)}\"\n)\n# yes... empty tuples exist in Python\nif value &lt; 0:\nraise ValueError(f\"unpack_to must be a positive int or None, provided {value}\")\n_validate_tuple(self.exec_function, value)\nself._unpack_to = value\n</code></pre>"},{"location":"nodes/#tawazi.node.node.ExecNode.id","title":"<code>id: Identifier</code>  <code>property</code>","text":"<p>The identifier of this ExecNode.</p>"},{"location":"nodes/#tawazi.node.node.ExecNode.tag","title":"<code>tag: Optional[TagOrTags]</code>  <code>writable</code> <code>property</code>","text":"<p>The Tag or Tags of this ExecNode.</p>"},{"location":"nodes/#tawazi.node.node.ExecNode.debug","title":"<code>debug: bool</code>  <code>writable</code> <code>property</code>","text":"<p>Whether this ExecNode is a debug Node. ExecNode can't be setup and debug simultaneously.</p>"},{"location":"nodes/#tawazi.node.node.ExecNode.setup","title":"<code>setup: bool</code>  <code>writable</code> <code>property</code>","text":"<p>Whether this ExecNode is a setup Node. ExecNode can't be setup and debug simultaneously.</p>"},{"location":"nodes/#tawazi.node.node.ExecNode.result","title":"<code>result: Union[NoValType, Any] = NoVal</code>  <code>instance-attribute</code>","text":"<p>Internal attribute to store the result of the execution of this ExecNode (Might change!).</p>"},{"location":"nodes/#tawazi.node.node.ExecNode.priority","title":"<code>priority: int</code>  <code>writable</code> <code>property</code>","text":"<p>The priority of this ExecNode.</p>"},{"location":"nodes/#tawazi.node.node.ExecNode.is_sequential","title":"<code>is_sequential: bool</code>  <code>writable</code> <code>property</code>","text":"<p>Whether <code>ExecNode</code> runs in sequential order with respect to other <code>ExecNode</code>s.</p>"},{"location":"nodes/#tawazi.node.node.ExecNode.executed","title":"<code>executed: bool</code>  <code>property</code>","text":"<p>Whether this ExecNode has been executed.</p>"},{"location":"others/","title":"Others","text":"<p>Equivalent of <code>and</code> wrapped in ExecNode.</p> Source code in <code>tawazi/_object_helpers.py</code> <pre><code>@xn\ndef and_(a: T, b: V) -&gt; Union[T, V]:\n\"\"\"Equivalent of `and` wrapped in ExecNode.\"\"\"\nreturn a and b\n</code></pre> <p>Equivalent of <code>or</code> wrapped in ExecNode.</p> Source code in <code>tawazi/_object_helpers.py</code> <pre><code>@xn\ndef or_(a: T, b: V) -&gt; Union[T, V]:\n\"\"\"Equivalent of `or` wrapped in ExecNode.\"\"\"\nreturn a or b\n</code></pre> <p>Equivalent of <code>not</code> wrapped in ExecNode.</p> Source code in <code>tawazi/_object_helpers.py</code> <pre><code>@xn\ndef not_(a: Any) -&gt; bool:\n\"\"\"Equivalent of `not` wrapped in ExecNode.\"\"\"\nreturn not a\n</code></pre> <p>         Bases: <code>str</code>, <code>Enum</code></p> <p>The strategy to use when an error is raised inside a function in a DAG.</p> Source code in <code>tawazi/errors.py</code> <pre><code>@unique\nclass ErrorStrategy(str, Enum):\n\"\"\"The strategy to use when an error is raised inside a function in a DAG.\"\"\"\n# supported behavior following a raised error\nstrict: str = \"strict\"  # stop the execution of the whole DAG\nall_children: str = \"all-children\"  # stop the execution of the all successors\npermissive: str = \"permissive\"  # continue the execution of the whole DAG\n</code></pre> <p>         Bases: <code>str</code>, <code>Enum</code></p> <p>The Resource to use launching ExecNodes inside the DAG scheduler a DAG.</p> <p>Resource can be either: 1. \"thread\": Launch the ExecNode in a thread (Default) 2. \"main-thread\": Launch the ExecNode inside the main thread, directly inside the main scheduler.</p> <p>Notice that when \"main-thread\" is used, some of the scheduler functionalities stop working as previously expected: 1. No new ExecNode will be launched during the execution of the corresponding ExecNode 2. If timeout is set on the corresponding ExecNode, it is not guaranteed to work properly.</p> Source code in <code>tawazi/consts.py</code> <pre><code>@unique\nclass Resource(str, Enum):\n\"\"\"The Resource to use launching ExecNodes inside the DAG scheduler a DAG.\n    Resource can be either:\n    1. \"thread\": Launch the ExecNode in a thread (Default)\n    2. \"main-thread\": Launch the ExecNode inside the main thread, directly inside the main scheduler.\n    Notice that when \"main-thread\" is used, some of the scheduler functionalities stop working as previously expected:\n    1. No new ExecNode will be launched during the execution of the corresponding ExecNode\n    2. If timeout is set on the corresponding ExecNode, it is not guaranteed to work properly.\n    \"\"\"\n# supported behavior following a raised error\nthread: str = \"thread\"\nmain_thread: str = \"main-thread\"\n</code></pre>"},{"location":"others/#tawazi.consts.Resource.thread","title":"<code>thread: str = 'thread'</code>  <code>class-attribute</code>","text":""}]}